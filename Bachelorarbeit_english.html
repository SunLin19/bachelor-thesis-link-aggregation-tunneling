<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:x="http://www.texmacs.org/2002/extensions" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <head>
    <title>No title</title>
    <meta content="TeXmacs 1.99.3" name="generator"></meta>
    <style type="text/css">
      body { text-align: justify } h5 { display: inline; padding-right: 1em }
      h6 { display: inline; padding-right: 1em } table { border-collapse:
      collapse } td { padding: 0.2em; vertical-align: baseline } .subsup {
      display: inline; vertical-align: -0.2em } .subsup td { padding: 0px;
      text-align: left} .fraction { display: inline; vertical-align: -0.8em }
      .fraction td { padding: 0px; text-align: center } .wide { position:
      relative; margin-left: -0.4em } .accent { position: relative;
      margin-left: -0.4em; top: -0.1em } .title-block { width: 100%;
      text-align: center } .title-block p { margin: 0px } .compact-block p {
      margin-top: 0px; margin-bottom: 0px } .left-tab { text-align: left }
      .center-tab { text-align: center } .balloon-anchor { border-bottom: 1px
      dotted #000000; outline:none;                  cursor: help; position:
      relative; }.balloon-anchor [hidden] { margin-left: -999em; position:
      absolute; display: none; }.balloon-anchor:hover [hidden] { position:
      absolute; left: 1em; top: 2em; z-index: 99; margin-left: 0; width:
      500px; display: inline-block; }.balloon-body { }.ornament  {
      border-width: 1px; border-style: solid; border-color:  black; display:
      inline-block; padding: 0.2em; } .right-tab { float: right; position:
      relative; top: -1em } 
    </style>
  </head>
  <body>
    <p>
      
    </p>
    <p>
      <p>
        
      </p>
      <table class="title-block">
        <tr>
          <td><font size="+2"><table class="title-block">
            <tr>
              <td><font size="+5"><b>Evaluation of an Approach for OSI Layer 3
              Link Aggregation Tunneling</b></font></td>
            </tr>
          </table><table class="title-block">
            <tr>
              <td><font size="+3"><b><p>
                
              </p><p>
                Discussion of Design, an Implementation 
              </p><p>
                and the Results of several Experiments 
              </p><p>
                in different Network Environments
              </p><p>
                
              </p></b></font></td>
            </tr>
          </table><p style="margin-top: 1em; margin-bottom: 1em">
            <div class="compact-block">
              <table class="title-block">
                <tr>
                  <td><p style="margin-top: 0.5em; margin-bottom: 0.5em">
                    <span style="margin-left: 0pt"></span>
                    <table style="display: inline; vertical-align: -0.55em">
                      <tbody><tr>
                        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-bottom: 0em; padding-top: 0em; width: 100%"><center>
                          <p>
                            <class style="font-variant: small-caps"> Richard Sailer</class>
                          </p>
                        </center></td>
                      </tr></tbody>
                    </table>
                  </p></td>
                </tr>
              </table>
            </div>
          </p><table class="title-block">
            <tr>
              <td><p>
                Universit&auml;t Augsburg
              </p><p>
                Lehrstuhl f&uuml;r Organic Computing
              </p><p>
                Bachelorarbeit im Studiengang Informatik
              </p><p>
                
              </p><p>
                
              </p><p>
                
              </p><p>
                
              </p><p>
                
              </p><p>
                
              </p><p>
                
              </p><p>
                Copyright &copy; 2016 Richard Sailer
              </p><p>
                Permission is granted to copy, distribute and/or modify this
                document under the terms of the <em>GNU Free Documentation
                License</em> (GFDL), 
              </p><p>
                Version 1.3
              </p></td>
            </tr>
          </table></font></td>
        </tr>
      </table>
      <p>
        
      </p>
      <p>
        
      </p>
      <p>
        
      </p>
    </p>
    <h1>Table of contents</h1>
    <div style="text-indent: 0em">
      <div class="compact-block">
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>1<span style="margin-left: 1em"></span>Introduction</b> <span style="margin-left: 5mm"></span> <a
          href="#auto-1">7</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>2<span style="margin-left: 1em"></span>OSI Layer 3 Link Aggregation Tunneling</b>
          <span style="margin-left: 5mm"></span> <a href="#auto-2">9</a>
        </p>
        <p>
          A Description of a OSI Layer 3 Link Aggregation Tunneling Network
          Setup and its Purpose <span style="margin-left: 5mm"></span> <a href="#auto-3">9</a>
        </p>
        <p>
          1<span style="margin-left: 1em"></span>OSI Layer 3 Link Aggregation Tunneling <span
          style="margin-left: 5mm"></span> <a href="#auto-4">9</a>
        </p>
        <p>
          1.1<span style="margin-left: 1em"></span>Mechanics <span style="margin-left: 5mm"></span> <a href="#auto-6">9</a>
        </p>
        <p>
          2<span style="margin-left: 1em"></span>Purpose <span style="margin-left: 5mm"></span> <a href="#auto-7">10</a>
        </p>
        <p>
          Implementations <span style="margin-left: 5mm"></span> <a href="#auto-8">10</a>
        </p>
        <p>
          1<span style="margin-left: 1em"></span>Multipath VPN <span style="margin-left: 5mm"></span> <a href="#auto-9">10</a>
        </p>
        <p>
          2<span style="margin-left: 1em"></span>MLVPN <span style="margin-left: 5mm"></span> <a href="#auto-10">11</a>
        </p>
        <p>
          3<span style="margin-left: 1em"></span>Viprinet Bonding <span style="margin-left: 5mm"></span> <a href="#auto-11">11</a>
        </p>
        <p>
          3.1<span style="margin-left: 1em"></span>Network Architecture (Fig. 1) <span style="margin-left: 5mm"></span>
          <a href="#auto-13">12</a>
        </p>
        <p>
          3.2<span style="margin-left: 1em"></span>Encapsulation (Fig. 2) <span style="margin-left: 5mm"></span>
          <a href="#auto-14">13</a>
        </p>
        <p>
          Related technologies <span style="margin-left: 5mm"></span> <a href="#auto-15">13</a>
        </p>
        <p>
          1<span style="margin-left: 1em"></span>Multipath TCP <span style="margin-left: 5mm"></span> <a href="#auto-16">13</a>
        </p>
        <p>
          2<span style="margin-left: 1em"></span>SCTP Multihoming <span style="margin-left: 5mm"></span> <a href="#auto-18">14</a>
        </p>
        <p>
          2.1<span style="margin-left: 1em"></span>SCTP Problems and Drawbacks <span style="margin-left: 5mm"></span>
          <a href="#auto-19">14</a>
        </p>
        <p>
          2.2<span style="margin-left: 1em"></span>Conclusion <span style="margin-left: 5mm"></span> <a href="#auto-20">14</a>
        </p>
        <p>
          3<span style="margin-left: 1em"></span>Internet-Connection-Load-balancing <span style="margin-left: 5mm"></span>
          <a href="#auto-21">14</a>
        </p>
        <p>
          3.1<span style="margin-left: 1em"></span>Comparison to OSI Layer 3 Link Aggregation
          <span style="margin-left: 5mm"></span> <a href="#auto-22">15</a>
        </p>
        <p>
          Advantages <span style="margin-left: 5mm"></span> <a href="#auto-23">15</a>
        </p>
        <p>
          Disadvantages <span style="margin-left: 5mm"></span> <a href="#auto-24">15</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>3<span style="margin-left: 1em"></span>First Explorative Experiments</b> <span
          style="margin-left: 5mm"></span> <a href="#auto-25">17</a>
        </p>
        <p>
          3.1<span style="margin-left: 1em"></span>The Testing Network <span style="margin-left: 5mm"></span> <a
          href="#auto-26">17</a>
        </p>
        <p>
          3.1.1<span style="margin-left: 1em"></span>Network Architecture and Overview <span
          style="margin-left: 5mm"></span> <a href="#auto-27">17</a>
        </p>
        <p>
          3.1.2<span style="margin-left: 1em"></span>Used Hardware <span style="margin-left: 5mm"></span> <a href="#auto-29">18</a>
        </p>
        <p>
          3.1.3<span style="margin-left: 1em"></span>Software Configurations <span style="margin-left: 5mm"></span>
          <a href="#auto-31">18</a>
        </p>
        <p>
          3.1.3.1<span style="margin-left: 1em"></span>Artificial Latency <span style="margin-left: 5mm"></span>
          <a href="#auto-32">18</a>
        </p>
        <p>
          3.1.3.2<span style="margin-left: 1em"></span>Setup of the Multipath VPN Software <span
          style="margin-left: 5mm"></span> <a href="#auto-34">19</a>
        </p>
        <p>
          3.2<span style="margin-left: 1em"></span>Results <span style="margin-left: 5mm"></span> <a href="#auto-35">19</a>
        </p>
        <p>
          3.2.1<span style="margin-left: 1em"></span>Default Linux Congestion Control: Same
          Latency <span style="margin-left: 5mm"></span> <a href="#auto-36">19</a>
        </p>
        <p>
          3.2.2<span style="margin-left: 1em"></span> Default Linux Congestion Control, 100ms
          and 200ms Latency <span style="margin-left: 5mm"></span> <a href="#auto-40">21</a>
        </p>
        <p>
          3.2.2.1<span style="margin-left: 1em"></span>Throughput Analysis <span style="margin-left: 5mm"></span>
          <a href="#auto-42">21</a>
        </p>
        <p>
          3.2.2.2<span style="margin-left: 1em"></span>Reordering Analysis <span style="margin-left: 5mm"></span>
          <a href="#auto-43">22</a>
        </p>
        <p>
          3.2.3<span style="margin-left: 1em"></span>Summary and Next Steps <span style="margin-left: 5mm"></span>
          <a href="#auto-44">22</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>4<span style="margin-left: 1em"></span>The TCP Protocol and its Vulnerability to
          Packet Reordering</b> <span style="margin-left: 5mm"></span> <a href="#auto-45">23</a>
        </p>
        <p>
          4.1<span style="margin-left: 1em"></span>A Short Description of the TCP Protocol <span
          style="margin-left: 5mm"></span> <a href="#auto-46">23</a>
        </p>
        <p>
          4.1.1<span style="margin-left: 1em"></span>Reliability <span style="margin-left: 5mm"></span> <a href="#auto-48">23</a>
        </p>
        <p>
          4.1.2<span style="margin-left: 1em"></span>Flow Control and Congestion Control <span
          style="margin-left: 5mm"></span> <a href="#auto-49">24</a>
        </p>
        <p>
          4.1.2.1<span style="margin-left: 1em"></span>The Sliding Window <span style="margin-left: 5mm"></span>
          <a href="#auto-50">24</a>
        </p>
        <p>
          4.1.2.2<span style="margin-left: 1em"></span>The Size of the Sliding Windows
          (Congestion Control and Flow Control) <span style="margin-left: 5mm"></span> <a href="#auto-52">24</a>
        </p>
        <p>
          4.1.3<span style="margin-left: 1em"></span>TCP Extensions: Fast Retransmit <span
          style="margin-left: 5mm"></span> <a href="#auto-53">25</a>
        </p>
        <p>
          4.1.3.1<span style="margin-left: 1em"></span>Changes on Receiver Site <span style="margin-left: 5mm"></span>
          <a href="#auto-54">25</a>
        </p>
        <p>
          4.1.3.2<span style="margin-left: 1em"></span>Changes on Sender Site <span style="margin-left: 5mm"></span>
          <a href="#auto-55">25</a>
        </p>
        <p>
          4.1.3.3<span style="margin-left: 1em"></span>Discussion of Advantages and
          Disadvantages <span style="margin-left: 5mm"></span> <a href="#auto-58">26</a>
        </p>
        <p>
          4.2<span style="margin-left: 1em"></span>Ways to Make the Linux TCP Implementation
          Less Vulnerable to Packet Reordering <span style="margin-left: 5mm"></span> <a href="#auto-59">26</a>
        </p>
        <p>
          4.2.1<span style="margin-left: 1em"></span>Configuration via <tt>/proc/sys/net/</tt>
          Switches <span style="margin-left: 5mm"></span> <a href="#auto-60">26</a>
        </p>
        <p>
          4.2.2<span style="margin-left: 1em"></span>A New TCP for Persistent Packet Reordering
          <span style="margin-left: 5mm"></span> <a href="#auto-61">27</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>5<span style="margin-left: 1em"></span>A Closer Analysis of the Multipath VPN
          Implementation</b> <span style="margin-left: 5mm"></span> <a href="#auto-62">29</a>
        </p>
        <p>
          5.1<span style="margin-left: 1em"></span>Involved Networking Resources <span style="margin-left: 5mm"></span>
          <a href="#auto-63">29</a>
        </p>
        <p>
          5.2<span style="margin-left: 1em"></span>The Event Driven Design <span style="margin-left: 5mm"></span>
          <a href="#auto-69">30</a>
        </p>
        <p>
          5.2.1<span style="margin-left: 1em"></span>The Perl POE Framework <span style="margin-left: 5mm"></span>
          <a href="#auto-70">30</a>
        </p>
        <p>
          5.3<span style="margin-left: 1em"></span>Working <span style="margin-left: 5mm"></span> <a href="#auto-71">31</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>6<span style="margin-left: 1em"></span>Seeking the Cause of Reordering</b> <span
          style="margin-left: 5mm"></span> <a href="#auto-72">35</a>
        </p>
        <p>
          6.1<span style="margin-left: 1em"></span>Multipath VPN Implementation <span style="margin-left: 5mm"></span>
          <a href="#auto-73">35</a>
        </p>
        <p>
          6.2<span style="margin-left: 1em"></span>Linux IO Buffering and Scheduling <span
          style="margin-left: 5mm"></span> <a href="#auto-74">35</a>
        </p>
        <p>
          6.2.1<span style="margin-left: 1em"></span>Avoidability of Operating System Buffering
          induced Reordering <span style="margin-left: 5mm"></span> <a href="#auto-75">35</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>7<span style="margin-left: 1em"></span>Refined Experiments</b> <span style="margin-left: 5mm"></span>
          <a href="#auto-76">37</a>
        </p>
        <p>
          7.1<span style="margin-left: 1em"></span>Test Setup and Hardware <span style="margin-left: 5mm"></span>
          <a href="#auto-77">37</a>
        </p>
        <p>
          7.1.1<span style="margin-left: 1em"></span>Differences to First Explorative
          Experiments <span style="margin-left: 5mm"></span> <a href="#auto-80">39</a>
        </p>
        <p>
          7.1.1.1<span style="margin-left: 1em"></span>Omitted &ldquo;Internet&rdquo; Gateway
          Routers IG0 and IG1 <span style="margin-left: 5mm"></span> <a href="#auto-81">39</a>
        </p>
        <p>
          7.1.1.2<span style="margin-left: 1em"></span>VLANs Instead of Physical Networks and
          Ethernet Cards <span style="margin-left: 5mm"></span> <a href="#auto-82">39</a>
        </p>
        <p>
          Tagged VLANs Explanation and Configuration Details <span style="margin-left: 5mm"></span>
          <a href="#auto-83">39</a>
        </p>
        <p>
          Throughput or Performance Impacts <span style="margin-left: 5mm"></span> <a href="#auto-84">39</a>
        </p>
        <p>
          7.1.1.3<span style="margin-left: 1em"></span>More Capable Hardware for MTC and MTS
          <span style="margin-left: 5mm"></span> <a href="#auto-85">40</a>
        </p>
        <p>
          7.1.2<span style="margin-left: 1em"></span>About the Traffic Used to Measure <span
          style="margin-left: 5mm"></span> <a href="#auto-86">40</a>
        </p>
        <p>
          7.2<span style="margin-left: 1em"></span>Pre Measurements <span style="margin-left: 5mm"></span> <a
          href="#auto-87">41</a>
        </p>
        <p>
          7.2.1<span style="margin-left: 1em"></span>Pre-Measurement without Tunneling Daemon, 1
          Link <span style="margin-left: 5mm"></span> <a href="#auto-88">41</a>
        </p>
        <p>
          7.2.2<span style="margin-left: 1em"></span>Pre-Measurement without Tunneling Daemon, 2
          Links <span style="margin-left: 5mm"></span> <a href="#auto-90">41</a>
        </p>
        <p>
          7.2.3<span style="margin-left: 1em"></span>Pre-Measurement Multipath-vpn, 2 Links,
          without tcpdump <span style="margin-left: 5mm"></span> <a href="#auto-92">41</a>
        </p>
        <p>
          7.3<span style="margin-left: 1em"></span>Results on CPU Performance (I) <span style="margin-left: 5mm"></span>
          <a href="#auto-94">42</a>
        </p>
        <p>
          7.3.1<span style="margin-left: 1em"></span>Comparison of CPU Time Usage on MTC and on
          MTS (I) <span style="margin-left: 5mm"></span> <a href="#auto-95">43</a>
        </p>
        <p>
          7.3.1.1<span style="margin-left: 1em"></span>Goals <span style="margin-left: 5mm"></span> <a href="#auto-96">43</a>
        </p>
        <p>
          7.3.1.2<span style="margin-left: 1em"></span>What we Did <span style="margin-left: 5mm"></span> <a href="#auto-97">43</a>
        </p>
        <p>
          7.3.1.3<span style="margin-left: 1em"></span>Results and Interpretation <span style="margin-left: 5mm"></span>
          <a href="#auto-98">43</a>
        </p>
        <p>
          7.3.2<span style="margin-left: 1em"></span>Correlation between Throughput and CPU Time
          (II) <span style="margin-left: 5mm"></span> <a href="#auto-100">45</a>
        </p>
        <p>
          7.3.2.1<span style="margin-left: 1em"></span>Goals <span style="margin-left: 5mm"></span> <a href="#auto-101">45</a>
        </p>
        <p>
          7.3.2.2<span style="margin-left: 1em"></span>What we Did <span style="margin-left: 5mm"></span> <a href="#auto-102">45</a>
        </p>
        <p>
          7.3.2.3<span style="margin-left: 1em"></span>Results and Interpretation <span style="margin-left: 5mm"></span>
          <a href="#auto-103">45</a>
        </p>
        <p>
          7.3.3<span style="margin-left: 1em"></span>Differentiated CPU Usage Analysis (III)
          <span style="margin-left: 5mm"></span> <a href="#auto-105">46</a>
        </p>
        <p>
          7.3.3.1<span style="margin-left: 1em"></span>Goals <span style="margin-left: 5mm"></span> <a href="#auto-106">46</a>
        </p>
        <p>
          7.3.3.2<span style="margin-left: 1em"></span>What we Did <span style="margin-left: 5mm"></span> <a href="#auto-107">46</a>
        </p>
        <p>
          7.3.3.3<span style="margin-left: 1em"></span>Results and Interpretation <span style="margin-left: 5mm"></span>
          <a href="#auto-108">46</a>
        </p>
        <p>
          7.4<span style="margin-left: 1em"></span>Results on Connection Stability (II) <span
          style="margin-left: 5mm"></span> <a href="#auto-110">47</a>
        </p>
        <p>
          7.4.1<span style="margin-left: 1em"></span>Goals <span style="margin-left: 5mm"></span> <a href="#auto-111">47</a>
        </p>
        <p>
          7.4.2<span style="margin-left: 1em"></span>What we did <span style="margin-left: 5mm"></span> <a href="#auto-112">47</a>
        </p>
        <p>
          7.4.3<span style="margin-left: 1em"></span>Results <span style="margin-left: 5mm"></span> <a href="#auto-113">47</a>
        </p>
        <p>
          7.4.4<span style="margin-left: 1em"></span>Evaluation and Interpretation <span style="margin-left: 5mm"></span>
          <a href="#auto-115">48</a>
        </p>
        <p>
          7.5<span style="margin-left: 1em"></span>Results of Changing Linux Internal TCP
          Options (III) <span style="margin-left: 5mm"></span> <a href="#auto-116">49</a>
        </p>
        <p>
          7.5.1<span style="margin-left: 1em"></span>Goals <span style="margin-left: 5mm"></span> <a href="#auto-117">49</a>
        </p>
        <p>
          7.5.2<span style="margin-left: 1em"></span>What we did <span style="margin-left: 5mm"></span> <a href="#auto-118">49</a>
        </p>
        <p>
          7.5.2.1<span style="margin-left: 1em"></span>Series 1: Only Changing Sender TCP
          Configuration <span style="margin-left: 5mm"></span> <a href="#auto-119">49</a>
        </p>
        <p>
          7.5.2.2<span style="margin-left: 1em"></span>Series 2: Changing Sender and Receiver
          Configuration <span style="margin-left: 5mm"></span> <a href="#auto-120">49</a>
        </p>
        <p>
          7.5.3<span style="margin-left: 1em"></span>Results and Interpretation <span style="margin-left: 5mm"></span>
          <a href="#auto-121">50</a>
        </p>
        <p>
          7.5.3.1<span style="margin-left: 1em"></span>Explanation <span style="margin-left: 5mm"></span> <a href="#auto-126">51</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>8<span style="margin-left: 1em"></span>Conclusion</b> <span style="margin-left: 5mm"></span> <a href="#auto-127">53</a>
        </p>
        <p>
          8.1<span style="margin-left: 1em"></span>Evaluation Summary and Optimisation
          Possibilities <span style="margin-left: 5mm"></span> <a href="#auto-128">53</a>
        </p>
        <p>
          8.2<span style="margin-left: 1em"></span>About the Application Domain <span style="margin-left: 5mm"></span>
          <a href="#auto-129">54</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>Appendix A<span style="margin-left: 1em"></span>Detailed Performance Measurement
          Results</b> <span style="margin-left: 5mm"></span> <a href="#auto-130">55</a>
        </p>
        <p>
          A.1<span style="margin-left: 1em"></span>MTS: 100ms_200ms latency <span style="margin-left: 5mm"></span>
          <a href="#auto-131">55</a>
        </p>
        <p>
          A.2<span style="margin-left: 1em"></span>MTC: 100ms_200ms latency <span style="margin-left: 5mm"></span>
          <a href="#auto-132">56</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>Appendix B<span style="margin-left: 1em"></span>Perl POE Framework Example</b>
          <span style="margin-left: 5mm"></span> <a href="#auto-133">59</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>Appendix C<span style="margin-left: 1em"></span>Content of the Accompanying
          Disk</b> <span style="margin-left: 5mm"></span> <a href="#auto-134">61</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>Appendix D<span style="margin-left: 1em"></span>Acknowledgements</b> <span style="margin-left: 5mm"></span>
          <a href="#auto-135">63</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>Bibliography</b> <span style="margin-left: 5mm"></span> <a href="#auto-136">65</a>
        </p>
        <p style="margin-top: 1em; margin-bottom: 0.5em">
          <b>List of figures</b> <span style="margin-left: 5mm"></span> <a href="#auto-137">67</a>
        </p>
      </div>
    </div>
    <h1 id="auto-1">Chapter 1<br></br>Introduction</h1>
    <p>
      In short and colloquial terms this work is about better internet. More
      precisely we evaluate, test and explain a concept and an implementation
      of a software which aims to provide a more reliable and faster internet
      connection. The concept is called &ldquo;OSI Layer 3 Link Aggregation
      Tunneling&rdquo;(OL3LAT) . The implementation we are testing and
      explaining is called &ldquo;Multipath VPN&rdquo;. OL3LAT works by
      aggregating several maybe slow or unreliable internet connections
      together to one fast reliable virtual internet connection.
    </p>
    <p>
      This work begins with a concept chapter, describing the general concept
      of OL3LAT and how it works in general. We explain a example network
      architecture which (in slightly changed form) will be used for all
      experiments in the later chapters. Following we will propose several
      implementations, two open source ones and one commercial. In a last part
      we will explain related technologies like load balancing and Multipath
      TCP and what distinguishes them from OL3LAT.
    </p>
    <p>
      The next chapter contains two Explorative experiments carried out with
      Multipath VPN to get a survey and feeling for it's performance, it's
      working and issues. Here we see that it's quite CPU time consuming and
      produces heavy packet reordering which makes congestion controlling
      difficult for TCP.
    </p>
    <p>
      In the following TCP background chapter we explain how TCP works and why
      packet reordering is critical for its performance. Also we present two
      ways to make Linux TCP less vulnerable to packet reordering, which will
      be tested in a later chapter.
    </p>
    <p>
      The next chapter will describe the implementation of Multipath VPN more
      closely. We explain the networking resources it uses: a TUN interface
      and UDP sockets and what they do. Also a more detailed description of
      the packet processing mechanics is provided.
    </p>
    <p>
      In the next chapter we get to the nub of the observed packet reordering.
      In the explorative experiments chapter we had eliminated different
      latencies as the cause of reordering. In the Multipath VPN
      implementation chapter we had eliminated Multipath VPN as guiltier. So
      there remains: the operating systems. With the help of several Linux
      kernel development books we will prove that the kernel does packet
      buffering before sending in a way a user space process can not control.
      So finally a user space process has no definitive control over the point
      in time when a packet gets set on the medium. That means that the buffer
      of one uplink interface can decide to buffer several packets whereas the
      other has buffered (let's say ten) packets and now send all of them at
      once. This causes reordering. In a last section we look for ways to
      force the immediate sending of a packet. The answer of a Linux kernel
      developer and maintainer to exactly this question then gives the
      underwhelming result: it's almost impossible. Avoiding operating system
      buffering induced reordering would mean intricately additional software
      development work in kernel space.
    </p>
    <p>
      In the penultimate chapter we present several refined experiments, whose
      design and structure contains the findings (and new questions) of the
      previous chapters. After a description of the hardware used and the
      design of the experiments we show the results of some pretests. The next
      sections belong to the results of the main experiments. Several
      experiments are done on performance, among others the connection between
      throughput and CPU time usage and a closer observation of the ratio
      between <em>system-time</em> and <em>user-time</em> of the needed CPU
      time. Surprisingly very much time is spent in user space. The next block
      of experiments will cover the connection failure tolerance of Multipath
      VPN. &ldquo;What happens if one of the internet connections goes
      down?&rdquo;. The result is less impressive then hoped the throughput
      stagnates to zero for 5 seconds until it (at least fast) regenerates. We
      offer supposition for the reason of this and in the conclusion also
      propose a solution. 
    </p>
    <p>
      The last block of refined experiments evaluates the efficiency of the
      two TCP option changes explained in the TCP chapter to make TCP less
      vulnerable to packet reordering. Again with not so impressive results.
      The throughput graphs do not look that different. The block ends with
      some research results and assumptions, why these options were net very
      effective in the case of Multipath VPN induced reordering.
    </p>
    <p>
      The conclusion shortly summarises the findings of the previous chapters.
      In general Multipath VPN works and provides link aggregation which
      increases network throughput. Nevertheless this implementation is still
      in its child shoes, the reliability feature works suboptimal and the
      high CPU time consumption as well as the induced reordering are
      problematic. Fortunately for almost all of these issues a solution or
      optimisation is possible. We shortly explain those possibilities and
      their advantages as well as their disadvantages. We end with a short
      discussion analysis of the application domain of OL3LAT technologies in
      general, using the distinction of network traffic categories provided in
      [<a href="#bib-Stevens:1993:TIP">Ste93</a>]. While OL3LAT is perfectly fitting for bulk data
      traffic applications, for interactive traffic it's still good but not
      perfectly suitable since the packet processing also introduce a little
      additional latency (throughout our experiments about 3-4 milliseconds).
    </p>
    <h1 id="auto-2">Chapter 2<br></br>OSI Layer 3 Link Aggregation Tunneling</h1>
    <p>
      
    </p>
    <h2 id="auto-3">A Description of a OSI Layer 3 Link Aggregation Tunneling Network
    Setup and its Purpose</h2>
    <p>
      This chapter will give a short description of the Multipath VPN network
      setup which this work centres around. An implementation of this setup
      will be used for the experiments in Chapter <a href="#chapter-experiments">3</a>. The main
      purpose of Multipath VPN is making it possible to use multiple internet
      up-links and therefore improving network performance and reliability.
      After the technical description a short comparison of the Multipath VPN
      approach to other similar technologies will complete the chapter.
    </p>
    <h3 id="auto-4">1<span style="margin-left: 1em"></span>OSI Layer 3 Link Aggregation Tunneling</h3>
    <p>
      Although using 1 to <var>n</var> internet gateways is possible for
      simplicity, in this example and for the experiments a 2-gateway setup
      will be used, built in the following structure:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/Multipath_vpn_szenario_en.png" height="311" width="780"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 2.1. </b><a id="auto-5"></a>Example of a 2-gateway Multipath
            Tunneling network setup.
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <h4 id="auto-6">1.1<span style="margin-left: 1em"></span>Mechanics</h4>
    <p>
      The <strong>Clients</strong>
      <strong>C0</strong>,<strong>C1</strong>&hellip; in the local network use
      the Multipath-VPN &ldquo;Client&rdquo; <strong>MTC</strong> as their
      standard-gateway unaware of any tunnelling happening after this hop.
    </p>
    <p>
      The <strong>Multipath-VPN Client MTC</strong> wraps the received
      segments in a custom packet-format and sends them (in default
      configuration via UDP) via <strong>IG0</strong> and <strong>IG1</strong>
      to <strong>MTS</strong>. The following splitting policies are possible:
    </p>
    <ul>
      <li>
        <p>
          Static ( 1 packet via IG0 ,1 via IG1, etc. see attached .conf file
          used in the experiment)
        </p>
      </li>
      <li>
        <p>
          By destination port, destination ip, source port/ip etc. 
        </p>
        <p>
          Anything what can be used for a iptables rule for marking packets.
        </p>
      </li>
    </ul>
    <p>
      The <strong>Internet Gateways IG0 and IG1</strong> know nothing of the
      Multipath-VPN setup. They only forward the UDP datagrams to their
      destination like any other unrelated UDP datagrams. Likewise work all
      the other routers on the path to <strong>MTS</strong>. It is a intended
      design decision that these nodes don't need any configuration or
      specific behaviour, because the network administrator of the Multipath
      VPN solution usually only has control over the nodes
      <strong>MTC</strong> and <strong>MTS</strong>.
    </p>
    <p>
      The <strong>Multipath VPN Server MTS</strong> unwraps the packets and
      enqueues them into his <tt class="verbatim">incoming</tt> chain. Doing this, he
      behaves like any NAT-Router, substituting the source address on any
      packet with his address and forwarding it according to his routing
      table.
    </p>
    <p>
      The <strong>destination Server ZS</strong> gets the packets, generated
      by the <strong>Client</strong>, like <strong>MTS</strong> has been his
      NAT internet-gateway, without noticing anything of the tunnelling.
    </p>
    <p>
      The <strong>Way back</strong> works similar: If <strong>MTS</strong>
      receives a packet on a port corresponding to an established connection
      which has been created from the Multipath-VPN, he forwards this packet
      to <strong>MTC</strong> via <strong>IG0</strong> and
      <strong>IG1</strong>. Thereto he does the usual NAT answer procedure
      (replacing the current destination IP(his) with the IP of the original
      sender) and routes the packet to the tun interface. The tun interface is
      created by the MTS software, which takes the packet, wraps it and
      settles the delivery to <strong>MTC</strong>. After <strong>MTC</strong>
      got the packets he unwraps them and forwards them in the local net.
    </p>
    <h3 id="auto-7">2<span style="margin-left: 1em"></span>Purpose</h3>
    <p>
      The main purpose of this technology the usage of multiple internet
      connections as <strong>one</strong> internet-gateway for a local network
      to maximise throughput and reliability. Furthermore Multipath VPN has
      the following purposes:
    </p>
    <ul>
      <li>
        <p>
          Possibility of Encryption for network traffic (from
          <strong>MTC</strong> to <strong>MTS</strong>)
        </p>
      </li>
      <li>
        <p>
          all other benefits of tunnelling
        </p>
        <p>
          (i.e.: the internet-gateway doing NAT (<strong>MTS</strong>) and
          therefore the IP address our packets have to the outside world is
          chose-able, which enables some anonymity and source-deception).
        </p>
      </li>
      <li>
        <p>
          Possibility of setting priorities for specific protocols. The free
          configurability of iptables gives the possibility to define very
          precisely which packets have to be delivered how.
        </p>
      </li>
    </ul>
    <p>
      
    </p>
    <h2 id="auto-8">Implementations</h2>
    <h3 id="auto-9">1<span style="margin-left: 1em"></span>Multipath VPN</h3>
    <p>
      Multipath VPN is a Open Source multipath tunneling solution implemented
      by Markus Schr&auml;der.
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.1. See: https://github.com/pRiVi/multipath-vpn , The full
                source code is also supplied together with this work in the
                directory: code.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.1"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.1">2.1</a></class></sup>
      It's written in Perl, but uses the Linux specific network configuration
      utility
      <strong>iptables</strong>
      and therefore currently only runs on GNU/Linux systems.
    </p>
    <p>
      Real world application experience with this software and some of its
      issues were the reason for the author for this bachelor thesis.
      Multipath VPN will be used for the experiments in the next chapters, as
      well as for a more detailed description of the implementation.
    </p>
    <p>
      Since the Multipath VPN source Code is quite condensed, undocumented and
      difficult to understand a restructured, refactored,commented fork
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.2. See: https://github.com/richi235/multipath-vpn , The full
                source code is also supplied together with this work in the
                directory: code.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.2"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.2">2.2</a></class></sup>
      has been created for usage in this thesis. All code examples used in
      later chapters will be taken from this fork.
    </p>
    <h3 id="auto-10">2<span style="margin-left: 1em"></span>MLVPN</h3>
    <p>
      MLVPN (MultiLink Virtual Public Network)
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.3. See: https://github.com/zehome/MLVPN ,  The full source
                code is also supplied together with this work in the
                directory: code.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.3"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.3">2.3</a></class></sup>
      is an open source implementation quite similar to Multipath VPN. While
      it basically uses the same basic principle of operation (running in user
      space, using tun/tap devices etc.
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.4. See the implementation section of the next chapter for
                details on this.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.4"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.4">2.4</a></class></sup>
      ) it seems quite more mature and rounded off. 
    </p>
    <p>
      Since it's written in C it runs on GNU/Linux and Free- and Open-BSD.
      There exist Debian packets and User documentation. It also supports
      privilege separation of the running binaries for security and
      encryption+authentication. Multipath VPN currently runs entirely as root
      and uses no encryption. So for actual use MLVPN is recommended whereas
      for studying and analysing Multipath VPN will be chosen, since for this
      a smaller less complicated code base is of advantage and security
      concerns are no central topic of this work.
    </p>
    <h3 id="auto-11">3<span style="margin-left: 1em"></span>Viprinet Bonding</h3>
    <p>
      Viprinet is a german company which sells multipath tunneling (or bonding
      as they call it) solutions to business customers. In this context
      &ldquo;Solution&rdquo; solution means:
    </p>
    <ul>
      <li>
        <p>
          Their hardware (own branded router boxes)
        </p>
      </li>
      <li>
        <p>
          Software
        </p>
      </li>
      <li>
        <p>
          Running and maintaining the remote server (end of the tunnel, MTS in
          previous diagrams)
        </p>
      </li>
    </ul>
    <p>
      Since this is a proprietary closed source product, closer research and
      description is not possible. Anyway there exists a german software
      patent[<a href="#bib-2013anordnung">Gmb13</a>] called &ldquo;Anordnung&#x02D9; zum
      &Uuml;bermitteln eines Datenstroms &uuml;ber geb&uuml;ndelte
      &#x02D9;Netzwerkzugangsleitungen, sowie Sende- und
      Empfangshilfsvorrichtung &#x02D9;daf&uuml;r&rdquo;.
    </p>
    <p>
      It describes the structure and mechanics of their product quite accurate
      and contains the following (hand drawn) diagram of their network
      architecture and encapsulation concept:
    </p>
    <p>
      
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/Viprinet_architecture_diagramm_(from_patent).png" width="650"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
            <font size="-1"><p>
              <b>Figure 2.2. </b><a id="auto-12"></a>Diagram of the viprinet
              network(Fig.1) and encapsulation(Fig.2) concepts from the
              patent. 
            </p></font>
          </p></td>
        </tr></tbody>
      </table>
    </p>
    <h4 id="auto-13">3.1<span style="margin-left: 1em"></span>Network Architecture (Fig. 1)</h4>
    <p>
      In <strong>Fig.</strong>1 of Figure 2 element 5 is called
      &ldquo;Sendehilfsvorrichtung&rdquo; (german for auxiliary sending
      appliance) and does roughly the same thing as MTC described below, which
      is: encapsulating packets and choosing transmission lines.
    </p>
    <p>
      The same applies for the &ldquo;Empfangshilfsvorrichtung&rdquo; (element
      6) which is therefore comparable to MTS. The big round circle in the
      middle (element 3) symbolises the internet. 
    </p>
    <h4 id="auto-14">3.2<span style="margin-left: 1em"></span>Encapsulation (Fig. 2)</h4>
    <p>
      As visible and described in [<a href="#bib-2013anordnung">Gmb13</a>] viprinet also uses
      encapsulation and tunneling of whole IP packets. The f() function
      visible symbolises encryption. In the patent text they recommend SSL/TLS
      but don't specify which encryption their product actually uses.
    </p>
    <h2 id="auto-15">Related technologies</h2>
    <h3 id="auto-16">1<span style="margin-left: 1em"></span>Multipath TCP</h3>
    <p>
      While Multipath TCP achieves roughly the same (sending data over
      multiple physical carriers), the network architecture and realisation
      differ, because the points <em>where</em> the multipath
      &ldquo;tunnel&rdquo; starts and ends differ. Using Multipath TCP the
      Hosts establishing the connection already use several internet
      connections, as visible in the following simplified diagram, taken from
      the official Multipath TCP presentation foils[<a href="#bib-mptcp-slides">Bon13</a>] :
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/Multipath-TCP-diagramm.png" width="750"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 2.3. </b><a id="auto-17"></a>Multipath TCP Network Layout
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      The fact that the connection-end points manage the multiple internet
      connections has several practical implications:
    </p>
    <ol>
      <li>
        <p>
          The clients or connection end points have to implement the new
          Multipath TCP protocol in their operating systems network stack.
        </p>
        <p>
          This implementation is currently not very widespread, at the time of
          this writing
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    2.5. 13.06.2015
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-2.5"></a>
          <sup><class style="font-style: normal"><a href="#footnote-2.5">2.5</a></class></sup>
          :
        </p>
        <ol>
          <li>
            <p>
              Microsoft Windows does not ship one per default and also no one
              seems to exist.
            </p>
          </li>
          <li>
            <p>
              There is a implementation for the Linux kernel, but it has not
              been merged into the mainline kernel
              <p>
                <font size="-1"><div align="justify">
                  <div style="margin-left: 0px">
                    <div style="margin-right: 0px">
                      <class style="font-style: normal"><p>
                        2.6. The multipath TCP project offers own prebuild
                        Linux kernels including the implementation for Debian
                        GNU/Linux, Ubuntu, Gentoo and several other popular
                        distributions. Also they offer patches for people who
                        build their own kernels.
                      </p></class>
                    </div>
                  </div>
                </div></font>
              </p>
              <span style="margin-left: 0em"></span>
              <a id="footnr-2.6"></a>
              <sup><class style="font-style: normal"><a href="#footnote-2.6">2.6</a></class></sup>
              and still seems to have some childhood diseases.
              <p>
                <font size="-1"><div align="justify">
                  <div style="margin-left: 0px">
                    <div style="margin-right: 0px">
                      <class style="font-style: normal"><p>
                        2.7. A short and informal test has shown that
                        multipath TCP often doesn't use the full bandwidth of
                        all existing internet connections. Also their seem to
                        be problems with NAT or OpenVPN tunnels visible from
                        several github issues.
                      </p></class>
                    </div>
                  </div>
                </div></font>
              </p>
              <span style="margin-left: 0em"></span>
              <a id="footnr-2.7"></a>
              <sup><class style="font-style: normal"><a href="#footnote-2.7">2.7</a></class></sup>
              
            </p>
          </li>
        </ol>
      </li>
      <li>
        <p>
          The connection end point needs access to all the internet uplinks
          available. While this makes sense for a mobile device containing
          Wifi and UMTS access technology for a office computer at a company
          this is impractical. Giving all computers direct access to all
          internet uplinks means additional wiring or at least additional
          configuration for using several virtual networks over one wire. 
        </p>
      </li>
    </ol>
    <h3 id="auto-18">2<span style="margin-left: 1em"></span>SCTP Multihoming</h3>
    <p>
      The SCTP protocol provides a feature called Multihoming which is
      comparable to the multipath functionalities of Multipath TCP. [<a href="#bib-beckecomparison">BAR+</a>]
      contains an extensive comparison of these two protocol and their
      multipath performance in an intercontinental test-bed between Germany
      and china. 
    </p>
    <p>
      SCTP is a reliable in-order OSI Layer 4 protocol comparable to TCP but
      message boundaries are preserved like in UDP. Also SCTP provides some
      other additional features, but they are out of scope for this work, for
      additional details on SCTP see: [
      <a href="#bib-RFC4960">Ste07</a>
      ] on
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.8. https://tools.ietf.org/html/rfc4960
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.8"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.8">2.8</a></class></sup>
    </p>
    <p>
      
    </p>
    <h4 id="auto-19">2.1<span style="margin-left: 1em"></span>SCTP Problems and Drawbacks</h4>
    <p>
      Contrary to it's progressive features there are some issues which make
      using SCTP for real world application difficult:
    </p>
    <ul>
      <li>
        <p>
          OSI Layer 4 protocol support in most cases has to be part of the
          operating system, Microsoft Windows
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    2.9. The official microsoft answer to this is: &ldquo;We
                    have not seen sufficient customer demand to add SCTP
                    support to Windows&rdquo;, see:
                    https://connect.microsoft.com/VisualStudio/feedback/details/651980/sctp-support
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-2.9"></a>
          <sup><class style="font-style: normal"><a href="#footnote-2.9">2.9</a></class></sup>
          and Mac OS X, two of the most wide spread operating systems for
          consumer computing have no built-in support for SCTP.
        </p>
      </li>
      <li>
        <p>
          For usage of SCTP with NAT, the Nat router has to understand SCTP,
          while support for UDP and TCP is built into most routers for SCTP
          this feature is missing in most hardware[<a href="#bib-sctp-nat">Ran</a>]. Since
          Nat is a very widely used feature this is a huge blocker for SCTP.
        </p>
      </li>
      <li>
        <p>
          To benefit from the advantages of SCTP userspace applications have
          to be rewritten to use the SCTP networking API of the operating
          system, which is a big effort for rewriting existing TCP software.
        </p>
      </li>
    </ul>
    <h4 id="auto-20">2.2<span style="margin-left: 1em"></span>Conclusion</h4>
    <p>
      Like Multipath TCP SCTP requires another network architecture than OSI
      Layer 3 Link Aggregation to work, so a direct comparison is not useful,
      since the fields of applications are different. And like Multipath TCP
      SCTP currently suffers from the low spread on desktop and consumer
      devices since every client has to support it for the multiple features
      to work. This is a point where (currently) link aggregation and the next
      technology &ldquo;Internet-connection-load-balancing&rdquo; have their
      advantages because using these, only the router of the local net has to
      know and use the several internet connections to make the bandwidth
      available to all.
    </p>
    <h3 id="auto-21">3<span style="margin-left: 1em"></span>Internet-Connection-Load-balancing</h3>
    <p>
      Another possibility to use multiple internet connections together is
      <strong>Internet-Connection-Load-balancing</strong>. It
      <strong>differs</strong> from Multipath-VPN in the following properties:
    </p>
    <ul>
      <li>
        <p>
          The Router decides for every to-be-established connection separately
          which network interface (for example DSL, DOCSIS etc.) shall be
          used. Once established all packets of this connection are sent and
          received through this interface.
        </p>
      </li>
      <li>
        <p>
          The packets are sent directly to their destination, their is no
          additional Gateway node (no <strong>MTS</strong>)
        </p>
      </li>
      <li>
        <p>
          Tunnelling using a specific container format does not happen.
        </p>
      </li>
    </ul>
    <p>
      Though these 2 network setups have <strong>one thing</strong> <strong>in
      common</strong>, which the previously presented Multipath-TCP does not
      share:
    </p>
    <p>
      In both configurations, the router(s) have full control over the packet
      delivery. In both scenarios it's possible to define protocol and address
      specific rules.
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                2.10. This property is widely used in real world applications.
                For example to swap high priority traffic on a dedicated
                internet connection. Often this is internet telephony or a
                separate line for ssh connections.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-2.10"></a>
      <sup><class style="font-style: normal"><a href="#footnote-2.10">2.10</a></class></sup>
    </p>
    <p>
      After the description of the Internet-Connection-Load-balancing
      technology the next section will give a discussion and comparison of
      these 2 technologies.
    </p>
    <h4 id="auto-22">3.1<span style="margin-left: 1em"></span>Comparison to OSI Layer 3 Link
    Aggregation</h4>
    <p>
      <a id="auto-23"></a><h5>Advantages</h5>
    </p>
    <ul>
      <li>
        <p>
          <strong>Lower complexity</strong>
        </p>
        <p>
          There is no tunnelling software running on client(MTC) or
          server(MTS), which has to wrap the packets in a own
          container-format. Therefore two sources of error are omitted.
        </p>
      </li>
      <li>
        <p>
          The <strong>CPU Load</strong> for the involved router(s) is much
          <strong>lower</strong>
        </p>
        <p>
          Like most other VPN daemons,the Multipath-VPN-Software is
          implemented as a user space process on Windows and UNIX. So for
          every single packet, which is put into the tun0 interface by the
          kernel, a context switch to the user space is necessary. Especially
          smaller ARM Routers with clock frequencies below 1 GHz this has
          proven as a network performance limiting fact in real world
          applications.
        </p>
        <p>
          This load disintegrates completely when using I.C.-Load-balancing
          the previously described separate routing of single connections can
          be taken out completely by the kernel, with no context switches
          necessary.
        </p>
      </li>
      <li>
        <p>
          <strong>No</strong> separate <strong>server</strong> as internet
          gateway (<strong>MTS</strong>) needed. 
        </p>
        <p>
          Which saves upkeep and maintenance effort.
        </p>
      </li>
    </ul>
    <p>
      So altogether I.C.-Load-balancing is less complex and therefore holds
      fewer sources of errors but has the following disadvantages: 
    </p>
    <p>
      <a id="auto-24"></a><h5>Disadvantages</h5>
    </p>
    <ul>
      <li>
        <p>
          <strong>Less dynamic</strong>
        </p>
        <p>
          Once A TCP or UDP Connection has been established via DSL or DOCSIS
          up-link <var>&alpha;</var> it can not be moved to up-link
          <var>&beta;</var>. 
        </p>
        <p>
          Therefore an optimal distribution of the traffic on all the internet
          uplinks is not always possible. Especially because at the point in
          time when the connection is established and the router has to decide
          which up-link to use, he does not &ldquo;know&rdquo; how much up-
          and downstream traffic this connection will need.
        </p>
        <p>
          Multipath VPN doesn't know this problem because the distribution
          happens &ldquo;deeper&rdquo; in the network stack.
          <strong>MTS</strong> anyway gets all the traffic and sends them out
          with his address, no matter via which <strong>IGx</strong> he got
          it.
        </p>
      </li>
      <li>
        <p>
          <strong>No dedicated encryption or anonymisation</strong> possible
        </p>
        <p>
          Also all the other advantages of tunnelling vanish
        </p>
      </li>
      <li>
        <p>
          <strong>Lower failure safety</strong>
        </p>
        <p>
          If a internet up-link breaks down, all the the TCP and UDP
          connections which used this up-link are affected and broken. This
          does not happen when using Multipath VPN and a internet up-link
          breaks down, here all the packets of an TCP or UDP connection can
          still be routed over the other uplinks, although they are now under
          a heavier load of course.
        </p>
      </li>
    </ul>
    <p>
      The extensive possibilities of configuration, which traffic goes through
      internet up-link, is a property both technologies share. Although with
      Multipath VPN being a bit more dynamic and flexible in that aspect.
    </p>
    <p>
      Finally, there must be said, that none technology is really superior.
      Depending on purpose, available hardware, and experience of the staff
      the right technology has to be chosen individually.
    </p>
    <h1 id="auto-25">Chapter 3<br></br>First Explorative Experiments</h1>
    <p>
      <a id="chapter-experiments"></a>
    </p>
    <h2 id="auto-26">3.1<span style="margin-left: 1em"></span>The Testing Network</h2>
    <p>
      The whole network was designed with the goal of recreating the real
      world application of multipath vpn as closely as possible, to make it
      possible to reproduce and analyse the throughput breakdowns noticed in
      application. This may explain some of the following design decisions.
    </p>
    <h3 id="auto-27">3.1.1<span style="margin-left: 1em"></span>Network Architecture and Overview</h3>
    <p>
      For the experiment an architecture similar to the previously presented
      generic network architecture has been chosen:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/test_network_architecture.png" height="263" width="713"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.1. </b><a id="auto-28"></a><a id="experiment-architecture"></a>The Network architecture
            of the Experiment
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      <div style="margin-left: 70.291850488812px">
        <p>
          <dt>
            Measuring point
          </dt>
          On these hosts
          <tt class="verbatim">tcpdump</tt>
          was executed, to save and store the relevant network traffic
          network traffic.
        </p>
      </div>
      <div style="margin-left: 70.291850488812px">
        <p>
          <dt>
            Delay Control
          </dt>
          On these nodes it was possible to add additional transmission delay
          for every Frame on OSI Layer 2. See
          <a href="#Creation_of_artificial_latency">3.1.3.1</a>
          for details how this is realised.
        </p>
        <p>
          <dt>
            Network segments
          </dt>
          The IP-addresses, sub-nets and routing were configured in a way, to
          achieve that every connected node-to-node pair (MTC&lt;-&gt;IG1,
          IG1&lt;-&gt;MTS, etc.) has a own IP-sub-net.
        </p>
        <p>
          This was done to make sure all involved nodes really
          <strong>route</strong> the datagrams through the network instead of
          only relaying Ethernet frames.
        </p>
        <p>
          The Linux network stacks treats these two cases differently in
          several ways (See figure <a href="#fig8">3.3</a> for a diagram of the
          decisions during the processing of incoming packets.)
        </p>
        <p>
          Because in actual real world application case routing happens in
          these nodes, several sub-nets and routing was chosen to the Linux
          kernel networking behaviour more close to that.
        </p>
        <p>
          <dt>
            Multipath VPN
          </dt>
          The Multipath VPN daemon was installed and running on MTC and MTS.
        </p>
        <p>
          <dt>
            The Traffic
          </dt>
          Was directed from the Client notebook (left) to ZS and consisting
          only of binary zeros (
          <tt class="verbatim">cat /dev/zero</tt>
          )
        </p>
      </div>
      <div style="margin-left: 70.291850488812px">
        <p>
          <dt>
            NAT
          </dt>
          MTS will Nat all the packets it forwards to ZS. Although it has
          absolutely no effect on TCP flow and congestion control at all, it
          was enabled to get this experiment more close to any application
          case. NAT is needed to ensure the Answer Packets of ZS chose the
          correct way back. It is intended they use the Multipath VPN for the
          back way.
        </p>
      </div>
    </p>
    <h3 id="auto-29">3.1.2<span style="margin-left: 1em"></span>Used Hardware</h3>
    <p>
      All nodes (except for the testers client notebook) were <class style="font-variant: small-caps">PC
      Engines</class> <tt class="verbatim">Alix2d2</tt> router boards running a <tt
      class="verbatim">Debian</tt> wheezy GNU/Linux Operating System with a slightly
      modified Linux Kernel. A short overview over the hardware specs used in
      the test: 
    </p>
    <dl>
      <p>
        <dt>
          CPU
        </dt>
        <dd>
          <p>
            500 MHz AMD Geode LX800 (AMDs Intel-Atom counterpart with
            comparable low power and Head losses)
          </p>
        </dd>
      </p>
      <p>
        <dt>
          RAM
        </dt>
        <dd>
          <p>
            256 MB DDR DRAM
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Network connectivity
        </dt>
        <dd>
          <p>
            2-3  Fast Ethernet Cards (depending on specific sub-model)
          </p>
        </dd>
      </p>
    </dl>
    <p>
      For more detailed info's, see the
      <em>vendor's product page</em>
      [
      <a href="#bib-alix-boxes-doc">Gmb</a>
      ]
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                3.1. <a href="">http://www.pcengines.ch/alix2d2.htm</a>
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-3.1"></a>
      <sup><class style="font-style: normal"><a href="#footnote-3.1">3.1</a></class></sup>
      .
    </p>
    <p>
      The following picture is a photography of the actual testing network and
      hardware during the experiment:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Anhang_CD_Daten/Bilder/Versuch_centered.jpg" width="700"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.2. </b><a id="auto-30"></a>Photography of the Breadboard
            construction. (A concrete realisation of the testing network
            architecture in Figure <a href="#experiment-architecture">3.1</a>)
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <h3 id="auto-31">3.1.3<span style="margin-left: 1em"></span>Software Configurations</h3>
    <h4 id="auto-32"><a id="Creation_of_artificial_latency"></a>3.1.3.1<span style="margin-left: 1em"></span>Artificial Latency</h4>
    <p>
      For the creation of the artificial Latency the Linux tool <tt class="verbatim">tc</tt>
      was used, together with the sub-module <tt class="verbatim">netem</tt>. As a
      concrete example the  command:
    </p>
    <pre class="verbatim" xml:space="preserve">
tc qdisc add dev eth0 root netem delay 100ms</pre>
    <p>
      would add an additional delay of 100ms to every frame <em>going out</em>
      through the network interface eth0. So to add a realistic delay to IG0
      and IG1 it was needed to set tc disciplines on both interfaces so every
      packet going through (in any direction) gets the same delay.
    </p>
    <p>
      <em>How tc and netem work</em>
      can be better understood with a little knowledge of how the Linux
      kernel handles packets. This simplified
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                3.2. The diagram had to be simplified, because it was much to
                big to layout. Mainly details about the iptables ad ebtables
                rooting chain were removed and infos how and when packets can
                jump from the link layer to the network layer processing. The
                <em>full image</em> is accessible via <a href="http://ebtables.netfilter.org/br_fw_ia/br_fw_ia.html">netfilter.org</a>
                (Section 11 detailed picture) [<a href="#bib-linux-packet-processing">dSF</a>] [<a href="#bib-linux-packet-processing-img">Sny</a>].
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-3.2"></a>
      <sup><class style="font-style: normal"><a href="#footnote-3.2">3.2</a></class></sup>
      diagram may help:
    </p>
    <p>
      
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/Iptables-PacketFlow_vereinfacht.png" height="295" width="700"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.3. </b><a id="auto-33"></a><a id="fig8"></a>Processing of Packets in
            the Linux Kernel (simplified) [<a href="#bib-linux-packet-processing">dSF</a>] [<a href="#bib-linux-packet-processing-img">Sny</a>]
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      As recognisable the evaluation of the Qdiscs (all red boxes) happens
      before and after anything else in the Linux kernel. Since the <tt class="verbatim">netem
      delay</tt> module only affects outgoing packets, only the two last red
      &ldquo;boxes&rdquo; are relevant, directly prior to passing the frames
      to the NIC. Every packet passing the 2nd red box will be held back for
      the specified time until it will be forwarded. While this method of
      delay creation is not completely similar to the real world application
      with several routers on the way to the MTS, all with their own
      buffering, it's quite comparable. With the delay happening outside the
      routing process it's more comparable to happening &ldquo;somewhere
      outside on the line or in an other router&rdquo; than some manipulation
      with iptables.
    </p>
    <h4 id="auto-34">3.1.3.2<span style="margin-left: 1em"></span>Setup of the Multipath VPN
    Software</h4>
    <p>
      Both Multipath VPN instances (MTC and MTS) were configured to use the
      two network interfaces belonging to the two different connecting NICs
      and ethernet wires with a ratio of 1:1. The concrete configuration files
      used can be found on the accompanying disk in the folder <tt class="verbatim">&quot;multipath-vpn_conf&quot;</tt>.
    </p>
    <h2 id="auto-35">3.2<span style="margin-left: 1em"></span>Results</h2>
    <h3 id="auto-36">3.2.1<span style="margin-left: 1em"></span>Default Linux Congestion Control: Same
    Latency</h3>
    <p>
      During the 82,6 seconds almost exactly 75.0 MByte of payload
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                3.3. Obtained this info from the ACK value of the last
                transmitted segment from ZS to MTS
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-3.3"></a>
      <sup><class style="font-style: normal"><a href="#footnote-3.3">3.3</a></class></sup>
      have been transferred from the testers notebook (called Client 0 = C0
      in the following text) to ZS with an average goodput of about 908
      kbyte/s. The CPU time usage during this experiment was continuous
      staggering between 99 and 100%. This is a graph of the complete I/O over
      time:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Anhang_CD_Daten/Experimente/first_experiment_series/Exp0__ig0+0ms__ig1+0ms/ZS_full_IOGraph_zentriert.png" height="200" width="540"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.4. </b><a id="auto-37"></a><a id="explorative-experiment-1"></a>Explorative Experiment 1:
            full bidirectional Network I/O Graph, Y-Axis in Bytes I/O
            (payload) per second
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      The worst traffic collapse happens around 20s so let's look more
      precisely what happens around this time:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Anhang_CD_Daten/Experimente/first_experiment_series/Exp0__ig0+0ms__ig1+0ms/ZS_full_IOGraph_0.1s_zoomed_in_zentriert.png" height="200" width="706"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.5. </b><a id="auto-38"></a>Explorative Experiment 1:
            bidirectional I/O Graph zoomed time-range : Second 16 to 28
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      Here <strong>2 facts</strong> attract attention:
    </p>
    <ol>
      <li>
        <p>
          The throughput is permanently staggering.
        </p>
      </li>
      <li>
        <p>
          Shortly before second 20 the throughput vanishes completely for
          about 0.4 seconds.
        </p>
        <p>
          To diagnose more precisely what happens in this time-span a closer
          look at the single packets and their attributes is needed:
        </p>
      </li>
    </ol>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Anhang_CD_Daten/Experimente/first_experiment_series/Exp0__ig0+0ms__ig1+0ms/wireshark_screenshot_resized.png" height="367" width="698"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 3.6. </b><a id="auto-39"></a>Wireshark screenshot: packet details
            in previously discussed time-range.
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      Here several facts get visible:
    </p>
    <ol>
      <li>
        <p>
          Packet reordering is happening.
        </p>
        <p>
          Wireshark marks all Packets he recognises as reordering-affected
          with black background and red font. As visible here this applies for
          the major part of packets in this little time span. Actually, this
          is true for the major part of packets of the whole transmission.
        </p>
        <p>
          This case is in particular interesting and extreme because here MTS
          gets 66 DUP-ACKs for frame 28592
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    3.4. Wireshark counts all frames of a recorded
                    transmission autonomous, 28592 is the wireshark-intern
                    frame-number of the packet for which ZS sends DUP-ACKs
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-3.4"></a>
          <sup><class style="font-style: normal"><a href="#footnote-3.4">3.4</a></class></sup>
          sequence, before he stops for a very long time and then
          re-transmits the queried next packet. 
        </p>
        <p>
          It should be mentioned, that the wireshark detection and marking of
          TCP Re-transmissions has (in contrast to the marking of the
          DUP-ACKs) some uncertainty. Every Time wireshark sees a packet
          containing a Sequence number lower than another previously seen
          packet of the same sender, wireshark marks it as Re-transmission.
          While the having-a-lower-sequence number fact can be a consequence
          of re-transmission, it can also be a consequence of reordering,
          which very likely is the case in this experiment in most cases.
        </p>
      </li>
      <li>
        <p>
          After second 19.47 (marked packet) the sender is not sending for
          about 0.5 seconds.
        </p>
        <p>
          Compared to all the other packet sending intervals in this
          transmission and as visible in the throughput diagrams this is a
          alarming huge time-span.
        </p>
      </li>
    </ol>
    <p>
      In this experiment we observed intensive packet reordering, although
      both link have the same, very low latency. For comparison the next
      section will show the throughput over time for a network using different
      latencies on the two links.
    </p>
    <h3 id="auto-40">3.2.2<span style="margin-left: 1em"></span> Default Linux Congestion Control, 100ms
    and 200ms Latency</h3>
    <p>
      During the next experiment, with a artificial delay of 100ms on one link
      and 200ms on the other link, we measured the throughput illustrated in
      figure <a href="#explor-exp-2">3.7</a>.
    </p>
    <p>
      
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-1.png" width="700"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
            <font size="-1"><p>
              <b>Figure 3.7. </b><a id="auto-41"></a><a id="explor-exp-2"></a>Explorative Experiment
              2: Bidirectional I/O Graph
            </p></font>
          </p></td>
        </tr></tbody>
      </table>
    </p>
    <h4 id="auto-42">3.2.2.1<span style="margin-left: 1em"></span>Throughput Analysis</h4>
    <p>
      In 87 seconds 48.229 Mbytes of payload were transferred, so the average
      goodput was 554 Kbyte/s. 
    </p>
    <p>
      This lower throughput is a consequence of the higher latency, to explain
      this a short excurse on TCP window sizes and and latencies is helpful:
    </p>
    <div class="ornament" style=";display:block;">
      <p>
        <p>
          <span class="ornament" style="background-color:white"><font color="white"><b><font color="red">Remark <class
          style="font-style: normal">3.1</class></font></b></font></span> <em>About TCP window
          sizes, Latency and Throughput</em>
        </p>
        <p>
          The maximal possible througput in a TCP connection depends on the
          maximal receive window size and the network latency in the following
          correlation:
        </p>
        <center>
          Maximal throughput in KBytes per second =
          <table class="fraction">
            <tr>
              <td style="border-bottom: solid 1px">TCP window Size in KBytes</td>
            </tr>
            <tr>
              <td>Latency in Seconds</td>
            </tr>
          </table>
        </center>
        <p>
          <em>Reason:</em>
        </p>
        <p>
          The maximal possible amount of data a TCP host can send at a point
          in time is the size of the Receiver's receive window. After he has
          sent this data (this can be several packets) he has to wait for one
          complete round trip time before he gets an ACK an can send new data.
          [<a href="#bib-tanenbaum2003computer">Tan03</a>] [<a href="#bib-isi_793rfc81">Ins81</a>] 
        </p>
      </p>
    </div>
    <p>
      
    </p>
    <p>
      If we want to calculate the estimated maximum possible throughput in
      this experiment according to this formula, we need to know the TCP
      window size. Fortunately this can be obtained from the traffic dump file
      it's about 98000 Bytes for over 98% percent of the packets, for the
      estimation we will use these values. For latency we will take the
      arithmetical median of the two links which is 150ms, remember this is
      just an raw estimation. 
    </p>
    <center>
      Throughput
      <sub>max</sub>
      =
      <table class="fraction">
        <tr>
          <td style="border-bottom: solid 1px">98.000 <span style="margin-left: 0.1em"></span>KBytes</td>
        </tr>
        <tr>
          <td>0.150 <var>s</var></td>
        </tr>
      </table>
      = 653.33 KByte/
      <var>s</var>
    </center>
    <p>
      653.33 KByte/s is a good estimation for the maximum throughput phases
      observed in the diagram, which occur several times but often get
      interrupted by reordering induced spikes.
    </p>
    <h4 id="auto-43">3.2.2.2<span style="margin-left: 1em"></span>Reordering Analysis</h4>
    <p>
      In general the relative throughput spread range in figure <a href="#explor-exp-2">3.7</a>
      is comparable to that in figure <a href="#explorative-experiment-1">3.4</a> (consider that the two
      diagrams do not use the same y-axis Byte/s resolution). There is one
      exception, the big spike in the first experiment at second 20. Since we
      do not know how reproducible this one is and if it could or could net
      happen in the second experiment as well we decided to ignore this
      difference.
    </p>
    <h3 id="auto-44">3.2.3<span style="margin-left: 1em"></span>Summary and Next Steps</h3>
    <p>
      Through this experiments we learned several facts about this OSI Layer 3
      Link Aggregation Tunneling implementation:
    </p>
    <ol>
      <li>
        <p>
          Massive Packet reordering even happens in a network with two uplinks
          of same latency so different latencies are <strong>not</strong> the
          only reason for packet reordering. Since the reordering induced
          jitter in a different-latencies network is comparable to that in a
          equal-latencies network, we even assume that different latencies
          only play a secondary rule as reordering cause.
        </p>
        <p>
          We will explain the results of further research on packet reordering
          in chapter <a href="#seeking-cause-of-reordering">6</a>.
        </p>
      </li>
      <li>
        <p>
          For low latencies and moderate receive window sizes Multipath VPN's
          throughput is CPU bound. For the hardware used the boundary in our
          experiment was at about 910 KByte/s. Several experiments and
          analyses on CPU performance will be presented in chapter <a href="#refined-experiments">7</a>
        </p>
      </li>
    </ol>
    <p>
      These explorative experiments gave us a good overview and clear
      direction what to examine next.
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <h1 id="auto-45">Chapter 4<br></br>The TCP Protocol and its Vulnerability to Packet
    Reordering</h1>
    <h2 id="auto-46">4.1<span style="margin-left: 1em"></span>A Short Description of the TCP
    Protocol</h2>
    <p>
      A TCP-Connection guarantees a reliable, ordered and error-checked
      connection between to sockets on two computers. In the OSI networking
      layer model TCP implements layer 4, the transport layer, in most cases
      TCP segments are contained in IP packets, and contain the data of the
      application layer. This chapter will give a short overview how TCP
      achieves this reliable and error-checked end-to-end connection, but at
      first a few terms.
    </p>
    <ul>
      <li>
        <p>
          Sockets are the endpoints of TCP connections
        </p>
      </li>
      <li>
        <p>
          Each connection between 2 computers is uniquely identified by the
          following four-tuple:
        </p>
        <p>
          (source-ip-address, source-port, destination-ip-address,
          destination-port)
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    4.1. For a practical example, on Linux Systems the command
                    <tt class="verbatim">netstat -t -n</tt> lists all the currently
                    established TCP connections together with this 4-tuple of
                    information. <tt class="verbatim">netstat -l -t -n</tt> lists all
                    the listening, not-yet-connection-established sockets.
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-4.1"></a>
          <sup><class style="font-style: normal"><a href="#footnote-4.1">4.1</a></class></sup>
          [
          <a href="#bib-isi_793rfc81">Ins81</a>
          ]
        </p>
      </li>
      <li>
        <p>
          A TCP connection is bi-directional, therefore sending and receiving
          data is possible for both hosts.
        </p>
      </li>
    </ul>
    <p>
      For the following explanations let's assume this example: 
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/simple_tcp_connection.png" height="112" width="381"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><p>
          <font size="-1"><p>
            <b>Figure 4.1. </b><a id="auto-47"></a>Example of a simple TCP Connection
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      For the simplicity we will only examine one direction, these of <font
      color="red">B</font> sending data to <font color="red">A</font>.
    </p>
    <h3 id="auto-48">4.1.1<span style="margin-left: 1em"></span>Reliability</h3>
    <p>
      So, how does a TCP implementation assure, all parts of the data-stream
      get delivered correctly? This is done using 2 mechanisms:
    </p>
    <dl>
      <p>
        <dt>
          Ack info
        </dt>
        <dd>
          <p>
            The receiver sends TCP segments containing an ACK flag and an ACK
            number telling how mach data and
            <em>until-which</em>
            byte of the data-stream he has received data.
            <p>
              <font size="-1"><div align="justify">
                <div style="margin-left: 0px">
                  <div style="margin-right: 0px">
                    <class style="font-style: normal"><p>
                      4.2. These &ldquo;Ack-Segments&rdquo; of course can
                      contain Payload data themselves, and so serve a double
                      purpose: transport Data of <class style="font-variant: small-caps"><font color="red">A</font></class>
                      and acknowledging successfully received Data of <font
                      color="red">B</font>. In most connections, this is the case. 
                    </p></class>
                  </div>
                </div>
              </div></font>
            </p>
            <span style="margin-left: 0em"></span>
            <a id="footnr-4.2"></a>
            <sup><class style="font-style: normal"><a href="#footnote-4.2">4.2</a></class></sup>
            <sup>,</sup>
            <p>
              <font size="-1"><div align="justify">
                <div style="margin-left: 0px">
                  <div style="margin-right: 0px">
                    <class style="font-style: normal"><p>
                      4.3. Whether for every received segment from the sender
                      an ACK segment is sent or for every second, third etc.
                      depends on the implementation, TCP extensions and the
                      fact how many segments have just been received.
                    </p></class>
                  </div>
                </div>
              </div></font>
            </p>
            <span style="margin-left: 0em"></span>
            <a id="footnr-4.3"></a>
            <sup><class style="font-style: normal"><a href="#footnote-4.3">4.3</a></class></sup>
          </p>
          <p>
            The sender starts a Timer for every sent segment.If this timer
            exceeds a certain timeout (the <em>RTO</em> Re-transmission
            Timeout) the segment is re-sent.[<a href="#bib-isi_793rfc81">Ins81</a>]
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Check-sums
        </dt>
        <dd>
          <p>
            Each TCP segment sent contains a check-sum calculated over the
            following information:
          </p>
          <ul>
            <li>
              <p>
                The &ldquo;Control&rdquo;-Data of a Pseudo-header containing
                the information of the TCP and the IP Header
              </p>
            </li>
            <li>
              <p>
                The Payload data
              </p>
            </li>
          </ul>
          <p>
            Therefore, if a TCP segment is received, its integrity can be
            checked and corresponding ACK info can be sent or not.[<a href="#bib-isi_793rfc81">Ins81</a>]
          </p>
        </dd>
      </p>
    </dl>
    <p>
      The next questions of relevance for the following chapters are:
      &ldquo;How much data (or how many segments) does <font color="red">B</font>
      send without getting an ACK and how does he keep track of all the ACKed
      and non-ACKed data?&rdquo;. These questions will be answered in the next
      chapter.
    </p>
    <h3 id="auto-49">4.1.2<span style="margin-left: 1em"></span>Flow Control and Congestion Control</h3>
    <h4 id="auto-50">4.1.2.1<span style="margin-left: 1em"></span>The Sliding Window</h4>
    <p>
      The sliding window algorithm is a concept for the
      <strong>sender</strong> to keep track of the <em>sent</em> segments and
      which of them are or are <em>not ACKed</em>. It contains all the
      segments, which are sent but not yet ACKed. Like the following figure
      illustrates:
    </p>
    <p>
      
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/sliding-window.jpg" height="336" width="444"></img></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
            <font size="-1"><p>
              <b>Figure 4.2. </b><a id="auto-51"></a>Illustration of the sliding window
              principle[<a href="#bib-sliding_window_webdoc">Kri</a>]
            </p></font>
          </p></td>
        </tr></tbody>
      </table>
    </p>
    <p>
      Here with every ACK message the window moves from left to right over the
      data stream, leaving behind successfully processed data. On every move
      of the sliding window, the sender sends the new segments which just came
      into the window.
    </p>
    <p>
      Using a sliding window brings the following benefits over simpler
      positive acknowledgement protocols: 
    </p>
    <ol>
      <li>
        <p>
          The sender can send several segments at once and doesn't have to
          wait for acknowledgements of every segment.
        </p>
      </li>
      <li>
        <p>
          The receiver can ACK several segments an once.
        </p>
      </li>
    </ol>
    <p>
      For these reasons TCP uses a sliding window concept for the senders.
    </p>
    <h4 id="auto-52">4.1.2.2<span style="margin-left: 1em"></span>The Size of the Sliding Windows
    (Congestion Control and Flow Control)</h4>
    <p>
      The size of the senders sliding window (in the following called
      <strong>snd.wnd</strong>) is <strong>not fixed</strong> but merely
      changing on almost every ACKed segment or timeout. The value of snd.wnd
      is calculated as the minimum of the following two values:
    </p>
    <dl>
      <p>
        <dt>
          rwin
        </dt>
        <dd>
          <p>
            The amount of bytes the receiver is currently willing to receive.
            This value is part of the TCP Header and is sent with every
            segment the receiver sends to the sender. The <strong>Flow
            Control</strong> part of TCP is done using this value.
          </p>
          <p>
            <strong>Example:</strong> If the application on receiver site,
            processing the data is quite slow and the data buffer of his
            operating system corresponding to the TCP socket get's near or on
            it's limit, the receiver can send a smaller rwin value to ensure
            he won't get &ldquo;to much&rdquo; data.
          </p>
        </dd>
      </p>
      <p>
        <dt>
          cwnd
        </dt>
        <dd>
          <p>
            A value maintained and used by the sender to do
            <strong>Congestion Control</strong>
            i.e. adjust the amount of sent data to the capacity of the
            network. A TCP Connection starts with a quite small value and
            increases it over time (details on this see slow start). For
            example if a packet loss is detected through timeout this value is
            set to it's initial (or halved depending on the implementation).
            This value is internally maintained by the Operating system TCP
            implementation and therefore not as easily visible in a network
            dump as rwin.
            <p>
              <font size="-1"><div align="justify">
                <div style="margin-left: 0px">
                  <div style="margin-right: 0px">
                    <class style="font-style: normal"><p>
                      4.4. For interested readers: On Linux based operating
                      systems since v2.6 the TCP implementation contains the
                      get_info() method to read out such values. The command
                      line tool ss uses this call. Using <tt class="verbatim">ss -t
                      -i</tt> one can read out the current sender-cwnd values
                      of all currently established TCP connection, the values
                      are displayed as &ldquo;number of segments possible to
                      send&rdquo; and therefore have to be multiplied with the
                      (also displayed) value of MSS +
                      <var>&asymp;</var>20Byte(Size of TCP Header).
                    </p></class>
                  </div>
                </div>
              </div></font>
            </p>
            <span style="margin-left: 0em"></span>
            <a id="footnr-4.4"></a>
            <sup><class style="font-style: normal"><a href="#footnote-4.4">4.4</a></class></sup>
          </p>
        </dd>
      </p>
    </dl>
    <h3 id="auto-53">4.1.3<span style="margin-left: 1em"></span>TCP Extensions: Fast Retransmit</h3>
    <p>
      The following Chapters explained the working of the plain TCP protocol
      as defined in RFC 793 in 1981. Since then many extensions and
      improvements have been discussed and implemented.
    </p>
    <p>
      One extension of significance for this work is <strong>Fast
      Retransmit</strong>:
    </p>
    <p>
      Fast Retransmit makes it possible to detect and correct packet loss much
      faster than only using the traditional re-transmission-timeout(RTO)
      method. This is achieved using the following mechanism:
    </p>
    <h4 id="auto-54">4.1.3.1<span style="margin-left: 1em"></span>Changes on Receiver Site</h4>
    <p>
      Let's assume the sender has just send 5 Segments with the (simplified)
      sequence numbers: 2,3,4,5,6 and segment 3 gets lost on the way to the
      receiver,so after a short time, the receiver get's the segments 2,4,5
      and 6. As a result, the receiver will at first send an ACK for segment
      2, and then an further <strong>duplicate-ACK</strong> (short DUPACK) for
      every further arriving segment being not the immediate following one of
      segment 2 [<a href="#bib-allman.paxson.stevens_2581rfc99">APS99</a>].
    </p>
    <h4 id="auto-55">4.1.3.2<span style="margin-left: 1em"></span>Changes on Sender Site</h4>
    <p>
      If the sender gets 4 ACK segments which fulfil one of the following
      requirements:
    </p>
    <ol>
      <li>
        <p>
          They all acknowledge the same Byte-stream position (ACK number)
        </p>
      </li>
      <li>
        <p>
          the 2nd, 3rd&hellip; acknowledge a Byte-stream position prior to the
          first one.
        </p>
      </li>
    </ol>
    <p>
      he immediately re-sends the segment directly following the first one and
      decreases <tt class="verbatim">cwnd</tt> to the half of its size [<a href="#bib-allman.paxson.stevens_2581rfc99">APS99</a>](a
      timeout in the old RTO mechanism would mean a resetting of <tt class="verbatim">cwnd</tt>
      to its start value which would have a much worse impact on the
      connection's bandwidth).
    </p>
    <p>
      The following diagram illustrates the described mechanics on Sender and
      receiver site:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-2.gif" height="410" width="631"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 4.3. </b><a id="auto-56"></a>Illustration of the Fast Retransmit
            mechanics.[frt]
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      At last an <strong>example</strong> of fast re-transmission observed in
      a real network:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"> <img src="Bachelorarbeit_english-3.png" height="184" width="616"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 4.4. </b><a id="auto-57"></a>Example of Fast Retransmit in
            network traffic. (Measured in wireshark)
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      A connection between the network hosts <strong>MTS</strong> and
      <strong>ZS</strong>. The left column indicates the sender of a segment.
      In this example <strong>MTS</strong> is bulk sending data to
      <strong>ZS</strong>(sender), while <strong>ZS</strong> is only receiving
      and sending ACK segments(receiver). 
    </p>
    <p>
      <strong>MTS</strong> starts with a data packet marked with Sequence
      number 2049 (the Seq=&hellip; field) of Length 1448 Bytes (the
      Len=&hellip; field). 
    </p>
    <p>
      <strong>ZS</strong> acknowledges the receiving of this whole segment
      send an ACK segment with ACK number 3497 ( = 2049 + 1448). Therefore the
      SEQ number <strong>ZS</strong> expects is <strong>3497</strong> (the
      2049th byte is the first of the 1448 delivered ones, therefore
      <strong>ZS</strong> just got all bytes up to including 3496). 
    </p>
    <p>
      But instead of 3497 <strong>ZS</strong> gets a segment with the SEQ
      number <strong>4945</strong>, so a part of the stream is missing and
      <strong>ZS</strong> sends the first DUPACK for 3497.
    </p>
    <p>
      The 2 subsequent arriving segments with SEQ  numbers greater than 4945
      are also answered with a DUPACK.
    </p>
    <p>
      Until finally <strong>MTS</strong> sends the correct segment containing
      the SEQ number 3497.
    </p>
    <h4 id="auto-58">4.1.3.3<span style="margin-left: 1em"></span>Discussion of Advantages and
    Disadvantages</h4>
    <p>
      While the fast retransmit extension brings a faster detection of packet
      losses and therefore faster re-transmissions it has one big problem: it
      can't distinguish between packet loss and packet reordering. It simply
      get's a number of segments like 2,5,6,4, diagnoses packet loss and
      triggers re-sending, also if packet 3 arrives shortly after that and
      actually the segments just got delivered out of order.
    </p>
    <p>
      Of course the receiver will send a ACK segment for segment 6 as fast as
      possible, after he received segment 4, but the 3 DUPACKS have already
      been sent so the sender will halve his <tt class="verbatim">cwnd</tt> and with
      this the connection bandwidth will lower.
    </p>
    <p>
      This type of reordering, without packet loss, called
      <em>persistent-reordering</em>, is the Achilles heel of the TCP Fast
      Retransmit extension as you will see in the following chapters.
    </p>
    <p>
      
    </p>
    <h2 id="auto-59">4.2<span style="margin-left: 1em"></span>Ways to Make the Linux TCP Implementation
    Less Vulnerable to Packet Reordering</h2>
    <p>
      When packet reordering induced throughput reduction becomes a problem,
      there a two ways to take countermeasures for a performance engineer. 
    </p>
    <p>
      One way is to avoid or reduce reordering. In Chapter <a href="#seeking-cause-of-reordering">6</a> we
      will try to understand the reasons for reordering using Multipath VPN
      and reason about possibilities to avoid it.
    </p>
    <p>
      Another way is to make the operating systems TCP implementation more
      resistant to packet reordering, in this section we will show and explain
      several measures to do this, some of them will be used and tested in
      section <a href="#linux-tcp-options">7.5</a>.
    </p>
    <h3 id="auto-60"><a id="tcp-switches-def"></a>4.2.1<span style="margin-left: 1em"></span>Configuration via <tt class="verbatim">/proc/sys/net/</tt>
    Switches</h3>
    <p>
      Linux offers several switches to configure TCP internals via the virtual
      <tt class="verbatim">proc</tt> filesystem. Two of these switches are relevant for
      TCP reordering resistance and will be explained in this section.
    </p>
    <dl>
      <p>
        <dt>
          /proc/sys/net/ipv4/tcp_reordering
        </dt>
        <dd>
          <p>
            The Linux programmer's manual[<a href="#bib-man-tcp">15</a>] on this option
            says:
          </p>
          <p>
            <div style="margin-left: 70.291740657624px">
              <div style="margin-right: 70.291740657624px">
                <div style="text-indent: 0em">
                  <p>
                    [tcp_reordering (integer; default: 3; since Linux 2.4)]
                  </p>
                </div>
              </div>
            </div>
            <p>
              
            </p>
            <div style="margin-left: 70.291740657624px">
              <div style="margin-right: 70.291740657624px">
                <div style="text-indent: 0em">
                  <p>
                    The maximum a packet can be reordered in a TCP packet
                    stream without TCP assuming packet loss and going into
                    slow start.  It is not advisable to change this number. 
                    This is a packet reordering detection metric designed to
                    minimise unnecessary back off and retransmits provoked by
                    reordering of packets on a connection. 
                  </p>
                </div>
              </div>
            </div>
          </p>
          <p>
            
          </p>
          <p>
            In section <a href="#linux-tcp-options">7.5</a> we will change this options to
            significant higher values and check the results.
          </p>
        </dd>
      </p>
      <p>
        <dt>
          /proc/sys/net/ipv4/tcp_fack
        </dt>
        <dd>
          <p>
            The Linux programmers's manual[<a href="#bib-man-tcp">15</a>] on this is quite
            short:
          </p>
          <p>
            <div style="margin-left: 70.291740657624px">
              <div style="margin-right: 70.291740657624px">
                <div style="text-indent: 0em">
                  <p>
                    tcp_fack (Boolean; default: enabled; since Linux 2.2)
                  </p>
                </div>
              </div>
            </div>
            <p>
              
            </p>
            <div style="margin-left: 70.291740657624px">
              <div style="margin-right: 70.291740657624px">
                <div style="text-indent: 0em">
                  <p>
                    Enable TCP Forward Acknowledgement support.
                  </p>
                </div>
              </div>
            </div>
          </p>
          <p>
            [<a href="#bib-sarolahti2002linux">Sar02</a>] gives a more elaborate explanation and
            reasoning in regards to packet reordering:
          </p>
          <p style="margin-top: 0.5em; margin-bottom: 0.5em">
            <div style="margin-left: 70.291740657624px">
              <div style="margin-right: 70.291740657624px">
                <div style="text-indent: 0em">
                  <p>
                    The Forward Acknowledgements (FACK) algorithm [14] takes a
                    more aggressive approach and considers the unacknowledged
                    holes between the SACK blocks as lost packets. Although
                    this approach often results in better TCP per- formance
                    than the conservative approach, it is overly aggressive if
                    packets have been reordered in the network, because the
                    holes be- tween SACK blocks do not indicate lost packets
                    in this case.
                  </p>
                </div>
              </div>
            </div>
          </p>
          <p>
            So another part of our experiments in section <a href="#linux-tcp-options">7.5</a>
            will be measuring the effect of turning out tcp_fack on the tcp
            throughput performance.
          </p>
        </dd>
      </p>
    </dl>
    <h3 id="auto-61">4.2.2<span style="margin-left: 1em"></span>A New TCP for Persistent Packet
    Reordering</h3>
    <p>
      In 2006 Stephan Bohacek, Katia Obraczka and several others released the
      paper &ldquo;A New TCP for Persistent Packet Reordering&rdquo; [<a href="#bib-bohacek2006new">BHL+06</a>]
      covering an alternative TCP algorithm which performs better than
      existing methods in case of persistent packet reordering.
    </p>
    <p>
      TCP-PR realizes this by not using duplicate acknowledgements (DUPACKs)
      at all and relying only on timers. Additionally it contains concepts to
      detect packet loses that belong to one &ldquo;loss event&rdquo; and
      accordingly reduces the send window size only once. 
    </p>
    <p>
      Since this concept sounded quite promising and there existed a
      provisional Linux kernel implementation we wanted to test this approach
      in our experiments. Unfortunately the code
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                4.5. Kindly provided by Katia Obraczka, thanks.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-4.5"></a>
      <sup><class style="font-style: normal"><a href="#footnote-4.5">4.5</a></class></sup>
      was developed for Linux 2.4.18, which in terms of kernel development
      progress is very old. We obtained a corresponding kernel and operating
      system (Debian woody from the Debian archives) but these were too old to
      support our test hardware. Even booting in a Xen virtual machine (HVM)
      did not work since the network interface provided by Xen was not
      supported by Linux 2.4.18. Therefore we decided (despite of the
      promising idea and simulation results in the paper) not to use TCP-PR
      for the experiments in this work.
    </p>
    <h1 id="auto-62">Chapter 5<br></br>A Closer Analysis of the Multipath VPN
    Implementation</h1>
    <h2 id="auto-63">5.1<span style="margin-left: 1em"></span>Involved Networking Resources</h2>
    <p>
      Multipath VPN uses two different kinds of network entities: TUN devices
      and UDP Sockets in the following structure.
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Diagramme/multipath_vpn_interna/multipath_vpn_interna.png" height="142" width="761"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 5.1. </b><a id="auto-64"></a>Diagram of the network interfaces
            and resources used by Multipath VPN
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      These entities have the following characteristics and usage:
    </p>
    <ul>
      <li>
        <p>
          The TUN or TAP interface [<a href="#bib-tuntap-linux">tun</a>]
        </p>
        <p>
          A TUN as well as a TAP interface is a network interface on a UNIX
          Operating System comparable to eth0 or or wlan0. 
        </p>
        <p>
          But while conventional network interfaces transport packets between
          the network stack and Hardware (like Ethernet chips or Wifi cards),
          TUN and TAP interfaces transport data between the network stack and
          user space processes:
        </p>
        <table style="width: 100%">
          <tbody><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em">   <table style="width: 100%">
              <tbody><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em"></td>
              </tr><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
              </tr><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em"><p>
                  <font size="-1"><p>
                    <b>. </b><a id="auto-65"></a>TUN or TAP interface
                  </p></font>
                </p></td>
              </tr></tbody>
            </table>                     <table style="width: 100%">
              <tbody><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em"></td>
              </tr><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
              </tr><tr>
                <td style="text-align: center; padding-left: 0em; padding-right: 0em"><p>
                  <font size="-1"><p>
                    <b>. </b><a id="auto-66"></a>Conventional Network Interface
                  </p></font>
                </p></td>
              </tr></tbody>
            </table></td>
          </tr><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
          </tr><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
              <font size="-1"><p>
                <b>Figure 5.2. </b><a id="auto-67"></a>A Simplified Comparison of
                tun/tap Interfaces to Conventional Network Interfaces
              </p></font>
            </p></td>
          </tr></tbody>
        </table>
        <p>
          MultipathVPN runs as one user space process and uses such a
          interface to get all the packets it has to handle. Therefore on
          startup the following is done:
        </p>
        <ol>
          <li>
            <p>
              A TUN interface is created and MultipathVPN associates itself
              with it.
            </p>
          </li>
          <li>
            <p>
              An IP route is configured which directs all (or the traffic
              which shall be tunneled according to the configuration file)
              packets to the TUN interface.
            </p>
          </li>
        </ol>
        <p>
          The <strong>difference</strong> between TUN and TAP interfaces is
          the type of packet it delivers to and from the user space process:
        </p>
        <table style="width: 100%">
          <tbody><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -1.1em">
              <tbody><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid; border-top: 1px solid">TAP Interface</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Delivers OSI Layer 2 Frames.</td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">TUN Interface</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">Delivers OSI Layer 3 Packets.</td>
              </tr></tbody>
            </table><table style="display: inline; vertical-align: -0.55em">
              <tbody><tr>
                <td></td>
              </tr></tbody>
            </table></td>
          </tr><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
          </tr><tr>
            <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
              <font size="-1"><p>
                <b>Table 5.1. </b><a id="auto-68"></a>TUN and TAP Interfaces
              </p></font>
            </p></td>
          </tr></tbody>
        </table>
        <p>
          
        </p>
        <p>
          Multipath VPN uses a TUN interface by which it get's IP packets
          which it wraps to the other end of the tunnel, where they get
          unwraped. There they are injected into the Operating Systems
          networking stack via a TUN interface and thereby processed and
          delivered to their destination. 
        </p>
      </li>
      <li>
        <p>
          UDP Sockets
        </p>
        <p>
          The tunnel <strong>endpoints</strong> are connected to each other
          via UDP connections. Each UDP connection ends in a UDP Socket on
          each host. The number of UDP connections available and used
          corresponds to the number of internet uplinks available.
        </p>
        <dl>
          <p>
            <dt>
              Sending tunnel part (MTC)
            </dt>
            <dd>
              <p>
                Each packet obtained from the <strong>TUN</strong> interface
                will be sent through one of the available UDP sockets.
                Remember that the UDP Sockets an operating system offers to
                user space expect data, wrap it into an UDP packet for you and
                send it to its destination. This means the <strong>IP
                packets</strong> will be sent as application layer payload,
                which means some attention has to be spent on the
                <strong>MTU</strong> of the different interfaces to avoid
                performance consuming fragmentation.
              </p>
            </dd>
          </p>
          <p>
            <dt>
              Receiving tunnel part (MTS)
            </dt>
            <dd>
              <p>
                If packets are received via a UDP socket the data multipath
                VPN assumes them to be IP datagrams and puts them into the TUN
                device on its site through which they will be delivered to
                their destination.
              </p>
            </dd>
          </p>
        </dl>
      </li>
    </ul>
    <h2 id="auto-69">5.2<span style="margin-left: 1em"></span>The Event Driven Design</h2>
    <p>
      Multipath VPN works event-driven which means:
    </p>
    <div class="ornament" style=";display:block;">
      <p>
        <p>
          <span class="ornament" style="background-color:white"><font color="white"><b><font color="blue">Definition <class
          style="font-style: normal">5.1</class></font></b></font></span> <i>event-driven
          (program)</i>
        </p>
        <p>
          <i><p>
            Per default an event driven program rests and waits for events
            from the &ldquo;outside&rdquo; (like an network packet from the
            TUN interface which has to be processed). 
          </p><p>
            For every considered event there exists an <strong>event
            handler</strong> function to handle this.
          </p></i>
        </p>
        <p>
          <i>Often there exists one <strong>main loop</strong> which iterates
          over all the possible sources of events and checks for new
          events.</i>
        </p>
      </p>
    </div>
    <h3 id="auto-70">5.2.1<span style="margin-left: 1em"></span>The Perl POE Framework</h3>
    <p>
      Also since Multipath VPN makes excessive use of the <class style="font-variant: small-caps">POE</class>
      Framework for event-driven programming in Perl a few words about this
      framework and it's concept will help to understand Multipath VPNs code.
      An overview over the most important entities and concepts in POE: 
    </p>
    <dl>
      <p>
        <dt>
          Kernel
        </dt>
        <dd>
          <p>
            The POE Kernel is the main loop this Framework provides the
            programmer. It gets all events as first entity and decides which
            event handler will be called. Also elementary scheduling and timer
            functionality is done by the kernel.
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Session
        </dt>
        <dd>
          <p>
            A session is a  set of event handler which belong semantically
            together. A session gets initialised on its definition and has one
            initialisation event-handler called <strong>_start</strong>.
            Furthermore each session has its own heap of private data.
            Additionally sessions build one semantic unit for the scheduling
            of the POE Kernel and events have to be addressed to a specific
            session. All this makes a session in some regards comparable to a
            task or even a process.
          </p>
        </dd>
      </p>
    </dl>
    <p>
      All this can be understood better with a simple example:
    </p>
    <p>
      With these knowledge of the concepts and frameworks used the foundations
      have been laid to understand the networking internals of Multipath VPN.
    </p>
    <h2 id="auto-71">5.3<span style="margin-left: 1em"></span>Working</h2>
    <p>
      Two types of Sessions of Multipath VPN are especially relevant for
      understanding the networking logic: The <em>TUN-Interface Session</em>
      and the <em>UDP-Socket Session</em>.
    </p>
    <p>
      In this example we will briefly show the most relevant code snippets of
      these sections and explain them.
    </p>
    <dl>
      <p>
        <dt>
          TUN-Interface Session
        </dt>
        <dd>
          <p>
            Handles receiving packets from and sending packets through the TUN
            Interface
          </p>
          <p>
            Contains 3 Event Handlers:
          </p>
          <dl>
            <p>
              <dt>
                _start
              </dt>
              <dd>
                <p>
                  Creates and configures the TUN interface.
                </p>
              </dd>
            </p>
            <p>
              <dt>
                got_packet_from_tun_device
              </dt>
              <dd>
                <p>
                  Simplified implementation code:
                </p>
                <p>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <p>
                          <tt class="verbatim">got_packet_from_tun_device =&gt; sub
                          {</tt>
                        </p>
                      </div>
                    </div>
                  </div>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <pre class="verbatim" xml:space="preserve">
  
    # read data from the tun device
    while ( sysread( $heap-&gt;{tun_device}, my $buf = &quot;&quot;, TUN_MAX_FRAME ) )
    
        <font color="#000080">#Decision begin</font>
        foreach my $sessid (
            sort( {( $sessions-&gt;{$a}-&gt;{tried} || 0 )
                  &lt;=&gt; ( $sessions-&gt;{$b}-&gt;{tried} || 0 ) }
              keys( %$sessions))
          )
        {
            if ($sessions-&gt;{$sessid}-&gt;{factor})
            { 
                $sessions-&gt;{$sessid}-&gt;{tried} += ( 1 / $sessions-&gt;{$sessid}-&gt;{factor} );
            }
            unless ( $nodeadpeer || $sessions-&gt;{$sessid}-&gt;{con}-&gt;{active} )
            { 
                next;
            }
            <font color="#000080">#Decision end</font>
            $_[KERNEL]-&gt;call( $sessid =&gt; &quot;send_through_udp&quot; =&gt; $buf );
            last;
        }
    }</pre>
                      </div>
                    </div>
                  </div>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <p>
                          <tt class="verbatim">},</tt>
                        </p>
                      </div>
                    </div>
                  </div>
                </p>
                <p>
                  Here we see at least some of the reasons for the high CPU
                  load. The code between the <font color="#008000">#Decission
                  begin</font> and <font color="#008000">#Decision end</font> comment
                  is responsible for deciding what UDP socket to use for
                  sending the current packet. It involves a call to the sort()
                  function, resolving of the keys of a hash-map ( keys()
                  function) and definition of an internal sorting order via
                  the <tt class="verbatim">&lt;=&gt;</tt> operator. Additionally every
                  <tt class="verbatim">-&gt;</tt> operator means reference resolving
                  overhead. All this could be implemented much cheaper using a
                  state machine, instead of sorting the elements of a hash map
                  every time and calculating factors using division.
                </p>
                <p>
                  One should also keep in mind that each <tt class="verbatim">-&gt;{&hellip;}</tt>
                  in Perl means a hash map lookup which also of course means
                  some key hashing and lookup overhead.
                </p>
                <p>
                  
                </p>
              </dd>
            </p>
            <p>
              <dt>
                put_into_tun_device
              </dt>
              <dd>
                <p>
                  Consists of the following code
                </p>
                <p>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <p>
                          <tt class="verbatim">        put_into_tun_device =&gt; sub
                          {</tt>
                        </p>
                      </div>
                    </div>
                  </div>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <pre class="verbatim" xml:space="preserve">
           [&hellip;] # POE specific argument passing

            # write data of $buf into the tun-device
            my $size = <font color="blue">syswrite</font>( $heap-&gt;{tun_device}, $buf );

            unless ( $size == length($buf) )
            { 
                print $size . &quot; != &quot; . length($buf) . &quot;\n&quot;;
            }</pre>
                      </div>
                    </div>
                  </div>
                  <div style="margin-left: 35.145870328812px">
                    <div style="text-indent: 0em">
                      <div class="compact-block">
                        <p>
                          <tt class="verbatim">        },</tt>
                        </p>
                      </div>
                    </div>
                  </div>
                </p>
                <p>
                  Fortunately this event handler is quite straightforward. The
                  most important part is the blue marked <font color="blue">syswrite()</font>
                  method call. Syswrite tells the Perl vm that it should not
                  use its own perlio buffering system but directly call the
                  system call <font color="blue">write()</font>. This event handler
                  gets called for every single packet received from each one
                  of the UDP socket sessions.
                </p>
              </dd>
            </p>
          </dl>
        </dd>
      </p>
      <p>
        <dt>
          UDP-Socket Session
        </dt>
        <dd>
          <p>
            Handles receiving packets from and sending packets through the UDP
            Socket
          </p>
          <p>
            From the 5 Event Handlers in this session the following 2 are
            relevant for our examination:
          </p>
          <dl>
            <p>
              <dt>
                got_data_from_udp
              </dt>
              <p>
                This event handler gets invoked by the POE framework if new
                data arrives on the udp socket this handler belongs too. It
                begins with a completely fine
              </p>
              <p style="margin-top: 1em; margin-bottom: 1em">
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">while ( defined(
                        $heap-&gt;{udp_socket}-&gt;recv( $curinput, 1600 ) )
                        )</tt>
                      </p>
                    </div>
                  </div>
                </div>
              </p>
              <p>
                loop header, calling the recv function on the socket. One
                performance issue we should note is, that this call again
                involves one hashmap lookup (<tt class="verbatim">-&gt;{..}</tt> in
                Perl) and two reference resolvings, which (if happening very
                often) are also problematic for performance.
              </p>
              <p>
                The next two lines have similar problems:
              </p>
              <p>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">$heap-&gt;{con}-&gt;{lastdstip}   =
                        $heap-&gt;{udp_socket}-&gt;peerhost();</tt>
                      </p>
                    </div>
                  </div>
                </div>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <pre class="verbatim" xml:space="preserve">
</pre>
                    </div>
                  </div>
                </div>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">$heap-&gt;{con}-&gt;{lastdstport} =
                        $heap-&gt;{udp_socket}-&gt;peerport();</tt>
                      </p>
                    </div>
                  </div>
                </div>
              </p>
              <p>
                The involve altogether 6 hashmap lookups, 8 de-referencing
                operations and 2 method calls. Since this happens for every
                packet received it looks like a good source of CPU time
                consumption.
              </p>
              <p>
                A few words about the last relevant part:
              </p>
              <p style="margin-top: 1em; margin-bottom: 1em">
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">$kernel-&gt;call( $tuntapsession
                        =&gt;&quot;put_into_tun_device&quot;, $curinput);</tt>
                      </p>
                    </div>
                  </div>
                </div>
              </p>
              <p>
                In essence <tt class="verbatim">$kernel-&gt;call()</tt> is equivalent to
                a function call. One could think that this function call means
                expensive copying of data, fortunately it doesn't since Perl
                by default uses call by value for all function calls.[<a href="#bib-christiansen2012programming">Chr12</a>]
              </p>
              <dt>
                send_through_udp
              </dt>
              <p>
                This event handler gets called from <tt class="verbatim">get_packet_from_tun_device</tt>
                and basically has the same issues as <tt class="verbatim">got_data_from_udp</tt>.
                Additionally it contains the following lines: 
              </p>
              <p>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">if ( $heap-&gt;{con}-&gt;{dstip} &amp;&amp;
                        $heap-&gt;{con}-&gt;{dstport} ) {</tt>
                      </p>
                    </div>
                  </div>
                </div>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <pre class="verbatim" xml:space="preserve">
    if ( my $dstip = inet_aton( $heap-&gt;{con}-&gt;{dstip} ) ) {
        $to = pack_sockaddr_in( $heap-&gt;{con}-&gt;{dstport}, $dstip );</pre>
                    </div>
                  </div>
                </div>
                <div style="margin-left: 35.145870328812px">
                  <div style="text-indent: 0em">
                    <div class="compact-block">
                      <p>
                        <tt class="verbatim">}</tt>
                      </p>
                    </div>
                  </div>
                </div>
              </p>
            </p>
          </dl>
          <p>
            which involve the <tt class="verbatim">inet_aton()</tt> function. <tt class="verbatim">inet_aton()</tt>
            converts a string containing nothing but a IP address to a binary
            representation which is used and needed by most syscalls [<a href="#bib-christiansen2012programming">Chr12</a>]
            [<a href="#bib-stevens2004unix">SFR04</a>]. The <tt class="verbatim">pack_sockaddr_in()</tt> call
            packs it's parameters into a data structure needed by the Perl
            socket implementation. These two function calls could be avoided
            if the conversion would be executed once and the result stored in
            the heap of the session. Even if the addresses change due to ip
            changes or other circumstances, it is possible to trigger an event
            for this and only reconvert the structures in this case, instead
            of for every packet to be sent.
          </p>
        </dd>
      </p>
    </dl>
    <p>
      In this chapter we looked closer at the implementation of Multipath VPN
      and the networking resources it uses. In addition to a closer
      understanding of its mechanics we indicated several code hunks which
      constitute potential performance problems.
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <h1 id="auto-72"><a id="seeking-cause-of-reordering"></a>Chapter 6<br></br>Seeking the Cause of Reordering</h1>
    <p>
      How does the reordering observed in the explorative experiments develop?
      Is it possible to prevent or avoid it? In this chapter we take a closer
      look at the Multipath VPN software and the Linux networking system. This
      will reveal, that the reordering is a exigent consequence of how
      Berkeley network sockets and the I/O Buffering system of Linux work.
    </p>
    <h2 id="auto-73">6.1<span style="margin-left: 1em"></span>Multipath VPN Implementation</h2>
    <p>
      The previous chapter explained the details of how Multipath VPN
      processes incoming and outgoing packets. This whole process is quite
      straightforward, packets are processed in the order Multipath VPN gets
      them from the tun interface (or a udp socket). Multipath VPN does not
      buffer several packets or actively reorder them. So we can safely assume
      the Multipath VPN software is not directly responsible for the
      reordering observed at the connection endpoints. 
    </p>
    <p>
      So Multipath VPN is not directly responsible but a technology it uses
      is, like the next section will explain.
    </p>
    <h2 id="auto-74">6.2<span style="margin-left: 1em"></span>Linux IO Buffering and Scheduling</h2>
    <p>
      Multipath VPN uses the Networking capabilities of the Linux kernel to
      send it's packet i.e. sockets, network interfaces.
    </p>
    <p>
      Linux handles the packet queue of every network interfaces separately.[
      <a href="#bib-linux-networking-overview">Riv</a>
      ]
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                6.1. The Raoul Rives Docs are some introductory slides about
                Linux kernel networking, accessible at: <a href="http://web.engr.illinois.edu/~caesar/courses/CS598.S11/slides/raoul_kernel_slides.pdf">his
                university page</a>. The diagram and description relevant here
                can be found in chapter &ldquo;Data Link Layer&rdquo; on slide
                20.
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-6.1"></a>
      <sup><class style="font-style: normal"><a href="#footnote-6.1">6.1</a></class></sup>
      [
      <a href="#bib-rosen2013linux">Ros13</a>
      ]
    </p>
    <p>
      There exists no concept like &ldquo;user space program x first put P1
      into eth0 then P2 into eth1 then P3 into eth0, therefore I best should
      process one packet of eth0, then one of eth1 and then one of eth0
      again&rdquo;. In the kernel the execution of each interface queue is
      handled separately and independent of such a &ldquo;user space
      interfaces hopping order&rdquo;.
    </p>
    <p>
      Even worse: from the point of an operating system scheduler often it
      makes sense to send several packets in the queue of an interface at once
      and then doing something else like file-system or terminal I/O or give
      control to another user space process. And then several milliseconds
      later send some packets in the queue of the other network interface.[<a
      href="#bib-rosen2013linux">Ros13</a>][<a href="#bib-haldar2010operating">HA10</a>]
    </p>
    <h3 id="auto-75">6.2.1<span style="margin-left: 1em"></span>Avoidability of Operating System
    Buffering induced Reordering</h3>
    <p>
      To avoid operating system buffering induced reordering it would be
      necessary to turn of the packet buffering the Linux kernel does. There
      are two buffers on the way of every packet through the kernel:[<a href="#bib-rosen2013linux">Ros13</a>][<a
      href="#bib-wu2007performance">WCB07</a>]
    </p>
    <ol>
      <li>
        <p>
          The socket buffer, belonging to the socket the application is using
        </p>
      </li>
      <li>
        <p>
          The ring buffer of the network interface
        </p>
      </li>
    </ol>
    <p>
      To avoid any buffering and therefore buffering induced reordering it
      would be necessary to set both of this buffers to size zero or avoid
      using them at all, which would in our case mean &ldquo;send a udp packet
      directly without any buffering in the kernel&rdquo;. To find out if it
      is possible to use the Linux kernel this way, we asked the kernel
      developer and subsystem maintainer Richard Weiberger via Email. His
      answer was (full mail including our quoted question):
    </p>
    <p>
      <div style="margin-left: 70.291740657624px">
        <div style="margin-right: 70.291740657624px">
          <div style="text-indent: 0em">
            <pre class="verbatim" xml:space="preserve">
Richard,

Am 16.02.2016 um 16:27 schrieb Richard Sailer:</pre>
          </div>
        </div>
      </div>
      <div style="margin-left: 70.291740657624px">
        <div style="margin-right: 70.291740657624px">
          <div style="text-indent: 0em">
            <p style="margin-top: 0.5em; margin-bottom: 0.5em">
              <div style="margin-left: 70.292179982376px">
                <div style="margin-right: 70.292179982376px">
                  <pre class="verbatim" xml:space="preserve">
Hello Mr. Weinberger,

I have a question regarding the kernel networking stack and NIC ringbuffers.

As a userspace process, is it possible to force immediate sending of a udp datagram?
(i.e. without any buffering of the datagram with following ones in the Ringbuffer of NIC) &lsquo;&lsquo;just to set the frame on the wire immediately&quot;?</pre>
                </div>
              </div>
            </p>
          </div>
        </div>
      </div>
      <div style="margin-left: 70.291740657624px">
        <div style="margin-right: 70.291740657624px">
          <div style="text-indent: 0em">
            <pre class="verbatim" xml:space="preserve">
if you're using sockets the packet will always go though the kernel networking
stack and my be queued int some buffers. Most likely within the buffer of the BSD socket.
To have the frame immediately you'd have to implement your own interface.
i.e. passing a packet directly to the network card driver or implement the driver in userspace using a framework like DPDK.

Thanks,
//richard


-- 
sigma star gmbh - Bundesstrasse 3 - 6111 Volders - Austria
ATU66964118 - FN 374287y
</pre>
          </div>
        </div>
      </div>
    </p>
    <p>
      So unfortunately, in the case of Multipath VPN, avoidance of the
      operating system buffering induced reordering would mean some quite
      intricately additional software development.
    </p>
    <h1 id="auto-76"><a id="refined-experiments"></a>Chapter 7<br></br>Refined Experiments</h1>
    <p>
      The results of the first explorative experiments as well the thoughts of
      the previous chapters clarified the necessity of further refined
      experiments. In this chapter we will analyse the following aspects of
      the implementation Multipath VPN: 
    </p>
    <ol>
      <li>
        <p>
          Context switches and detailed CPU statistics in relation to
          throughput
        </p>
      </li>
      <li>
        <p>
          Stability of connection and throughput when one link goes down
        </p>
      </li>
      <li>
        <p>
          The effect of changing several Linux internal TCP options on
          throughput
        </p>
      </li>
    </ol>
    <p>
      Each of this aspect got his own experiment, often several, at the
      beginning of each experiment we will explain what we changed in the
      configuration, what we wanted to measure and what we measured. 
    </p>
    <p>
      Especially for measuring the correlation between CPU speed and
      throughput other, more capable computers were necessary for the tunnel
      entrance and exit (MTC and MTS), the next section descries the hardware
      used, the test setup and the differences to the first explorative
      experiments.
    </p>
    <h2 id="auto-77"><a id="general-test-setup"></a>7.1<span style="margin-left: 1em"></span>Test Setup and Hardware</h2>
    <p>
      The following picture shows the hardware and test setup used:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bilder/20160211_005.jpg" width="700"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.1. </b><a id="auto-78"></a>Photography of the refined
            Experiment Setup
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      While this picture is very illustrative it does not show very much
      detail about routing or network configuration, so we included the
      following diagram to show the network architecture: 
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><p style="margin-top: 1em; margin-bottom: 1em">
          <div style="margin-left: 35.145870328812px">
            <div style="text-indent: 0em">
              <div class="compact-block">
                <pre class="verbatim" xml:space="preserve">
          (Old notebook)
         +-------------+
         |    (C0)     |
         |   Sender    |              
         |             |              
         |   eth0.12   |
         | 10.1.1.5    |
         +----+--------+
              |
              |
              |VLAN12
              |
              |
  +-----------+----------------+
  |       10.1.1.1             |
  |         eth1.12            |
  |                            |
  |          MTC               |
  |    IG1            IG0      |
  |   eth1.6          eth1.5   |
  |  10.2.2.1       10.3.3.1   |
  +----+--------------+--------+
       |              |
       |              |
       |VLAN6         | VLAN5
       |              |
       |              |
  +----+--------------+--------+
  |  10.2.2.2       10.3.3.2   |
  |   eth2.6          eth2.5   |
  |                            |
  |           MTS              |
  |                            |
  |           eth2.13          |
  |         10.4.4.2           |
  +-------------+--------------+
                |
                |
                | VLAN13
                |
                |
         +------+------+
         |   10.4.4.3  |
         |     eth0.13 |
         |             |              
         |  Receiver   |              
         |    (ZS)     |
         +-------------+
            (Thinkpad) </pre>
              </div>
            </div>
          </div>
        </p></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.2. </b><a id="auto-79"></a><a id="ref-net-arch"></a>Ascii art diagram of the
            refined test network architecture
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      In Figure <a href="#ref-net-arch">7.2</a> boxes are computers while lines are VLAN
      connections (more on these in section <a href="#vlan-conf">7.1.1.2</a>). The gentle
      reader may have noticed that two computers from the first explorative
      experiment were omitted: The routers of the internet gateways IG0 IG1.
    </p>
    <h3 id="auto-80">7.1.1<span style="margin-left: 1em"></span>Differences to First Explorative
    Experiments</h3>
    <p>
      Mainly because of practical and time constraint reasons several
      differences to the first explorative experiments were chosen.
    </p>
    <h4 id="auto-81">7.1.1.1<span style="margin-left: 1em"></span>Omitted &ldquo;Internet&rdquo; Gateway
    Routers IG0 and IG1</h4>
    <p>
      At our current level of understanding of OSI Layer 3 Link aggregation,
      the two router computers used in the first explorative experiments were
      omittable and their existence had no relevant effect on the results.
      Additionally The results of the first experiments (which are very
      similar to the explorative ones in design on purpose) affirm this. 
    </p>
    <p>
      The theoretical background for this decision is the following: 
    </p>
    <ol>
      <li>
        <p>
          The latency created by them is almost similar and negligible low.
        </p>
        <p>
          A small check using ping in the explorative experiments measured
          about 0.8 ms.
        </p>
      </li>
      <li>
        <p>
          Creating artificially latency is still possible without them.
        </p>
        <p>
          By lying the same <tt class="verbatim">tc add &hellip;</tt> configuration on
          the output and input VLAN devices on MTC and MTS.
        </p>
      </li>
    </ol>
    <h4 id="auto-82"><a id="vlan-conf"></a>7.1.1.2<span style="margin-left: 1em"></span>VLANs Instead of Physical
    Networks and Ethernet Cards</h4>
    <p>
      In the explorative Experiments MTC and MTS had 3 NICs and 3 LAN ports
      each, which were necessary (1 for connection to C0/ZS, 1 for IG0 and 1
      for IG1). For the refined experiments it was not possible to recreate
      this configuration since not enough PCI-e network cards were available
      to extend the provided computers. Therefore we used tagged VLANs
      according to IEEE 802.1q to create 3 virtual OSI Layer 2 networks for
      each NIC.
    </p>
    <p>
      <a id="auto-83"></a><h5>Tagged VLANs Explanation and Configuration Details</h5>
    </p>
    <p>
      Using VLAN every ethernet frame contains an additional attribute its <tt
      class="verbatim">vlanid</tt> or <tt class="verbatim">vid</tt>. When a adequate configured
      Linux host receives a ethernet frame containing such a <tt class="verbatim">vid</tt>
      he automatically associates this frame with the belonging virtual
      network interface. So in our experiment every host had 3 virtual network
      interfaces, each one receiving and sending ethernet frames containing
      complying <tt class="verbatim">vid</tt>s. So for example the network interface <tt
      class="verbatim">eth2.6</tt> was a virtual network interface on top of <tt class="verbatim">eth2</tt>
      sending and receiving frames of the VLAN <tt class="verbatim">6</tt>.
    </p>
    <p>
      Additionally the switch was configured to know the <tt class="verbatim">vid</tt>s
      and to switch the right ethernet frames to the right ports. This is why
      in the picture you see only 4 ethernet wires, 2 wires from MTC and MTS
      to the switch, each one transporting frames of 3 different vlans. The
      two wires between C0 or ZS and the switch only transported frames of 1
      vlan. 
    </p>
    <p>
      Additional details on the vlan configuration are noted in Figure
      <a href="#ref-net-arch">7.2</a>
      . A good explanation for a deeper understanding of VLANs can be found in
      [
      <a href="#bib-tanenbaum2003computer">Tan03</a>
      ]. Howtos for concrete usage and configuration are available on the
      documentation websites of most Linux distributions, for example
      <p>
        <font size="-1"><div align="justify">
          <div style="margin-left: 0px">
            <div style="margin-right: 0px">
              <class style="font-style: normal"><p>
                7.1. https://wiki.ubuntu.com/vlan
              </p></class>
            </div>
          </div>
        </div></font>
      </p>
      <span style="margin-left: 0em"></span>
      <a id="footnr-7.1"></a>
      <sup><class style="font-style: normal"><a href="#footnote-7.1">7.1</a></class></sup>
      .
    </p>
    <p>
      <a id="auto-84"></a><h5>Throughput or Performance Impacts</h5>
    </p>
    <p>
      For the experiments it was important to know the performance and
      Throughput impact of using tagged VLANs. It can be broke down to the
      following facts:
    </p>
    <dl>
      <p>
        <dt>
          Throughput
        </dt>
        <dd>
          <p>
            Since all 3 virtual network interface of MTC and MTS base on one
            physical gigabit ethernet network interface their maximal combined
            throughput is of course limited by the physical interfaces'
            maximal throughput. To rule out this fact would become the
            bottleneck we did some pretests to measure the pure throughput of
            the interface (traffic between MTS and MTC, no tunneling) and
            fortunately the final throughput of Multipath VPN was always less
            than 1/2
            <p>
              <font size="-1"><div align="justify">
                <div style="margin-left: 0px">
                  <div style="margin-right: 0px">
                    <class style="font-style: normal"><p>
                      7.2. Imagine 10 Frames reach MTC via eth1.12, after
                      Multipath VPN has divided them 5 leave through eth1.5
                      and 5 leave to eth1.6. So in the end 10 frames have
                      reached and 10 frames (=20) have left MTC to tunnel a
                      payload of 10 frames. So the throughput MTC can
                      transport (physically) is half of the throughput the
                      ethernet interface can handle since every frame passes
                      this interface twice.
                    </p></class>
                  </div>
                </div>
              </div></font>
            </p>
            <span style="margin-left: 0em"></span>
            <a id="footnr-7.2"></a>
            <sup><class style="font-style: normal"><a href="#footnote-7.2">7.2</a></class></sup>
            of this pure throughput. So in our experiments the VLAN
            throughput never was the bottleneck. For the details on this see
            the pre-measurements in chapter
            <a href="#pre-measurements">7.2.1</a>
            .
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Performance
        </dt>
        <dd>
          <p>
            One could think that parsing and understanding the <tt class="verbatim">vid</tt>s
            for every single frame means additional CPU load. Fortunately it
            doesn't since the NICs used in MTC and MTS supported VLAN
            offloading (verified via the ethernet configuration utility <tt
            class="verbatim">ethtool</tt>), which means all the CPU-intensive vlan
            handling was done by the NIC hardware.
          </p>
        </dd>
      </p>
    </dl>
    <p>
      So fortunately for our experiments the choice of another OSI Layer 2
      switching method had no measurable effect on any of the results.
    </p>
    <h4 id="auto-85">7.1.1.3<span style="margin-left: 1em"></span>More Capable Hardware for MTC and
    MTS</h4>
    <p>
      In the explorative experiments we discovered that the network throughput
      of Multipath VPN is CPU-bound. To be more precise bound to the CPU
      performance of the computers used for MTC and MTS in the network, these
      computers had a permanent CPU load of 100%, measured with the unix
      command top. Compared to this on the C0 and ZS computers the CPU load
      was about 10% i.e. no significant bottleneck.
    </p>
    <p>
      To measure how significant CPU performance is for throughput in the
      refined experiments several changes were chosen for the refined
      experiments:
    </p>
    <ol>
      <li>
        <p>
          We used computers with much more powerful CPUs: Intel Core i7-2600
          (4 real cores, 8 hyper threading cores with 3,4 GHz each), instead
          of the 500 MHz single-core AMD Geode low power processor in the
          first experiments.
        </p>
      </li>
      <li>
        <p>
          We did extra measurements for CPU load and CPU usage details using
          the tool <tt class="verbatim">pidstat</tt>
        </p>
      </li>
    </ol>
    <p>
      Those two measures gave us extensive possibilities to measure the CPU
      usage behaviour of Multipath VPN.
    </p>
    <h3 id="auto-86">7.1.2<span style="margin-left: 1em"></span>About the Traffic Used to Measure</h3>
    <p>
      The traffic used to measure had the following properties worth
      mentioning:
    </p>
    <ol>
      <li>
        <p>
          It's One-directional bulk traffic
        </p>
        <p>
          So none of the following experiments can tell any information on the
          capabilities of Multipath VPN for interactive Applications over TCP.
          
        </p>
        <p>
          On the other hand, this way we simulate a very common use case: the
          download of one single file from a fast server.
        </p>
        <p>
          
        </p>
      </li>
      <li>
        <p>
          It's One singular TCP connection
        </p>
        <p>
          This property has 2 reasons:
        </p>
        <ol>
          <li>
            <p>
              It's much easier to create one connection, than to create
              several.
            </p>
          </li>
          <li>
            <p>
              It's much easier to observe and analyse the sequence numbers and
              details of one connection than of several.
            </p>
          </li>
        </ol>
        <p>
          
        </p>
      </li>
      <li>
        <p>
          It's entirely consisting of binary zeroes.
        </p>
        <p>
          Created by the command <tt class="verbatim">nc &lt;target-ip&gt;
          &lt;target-port&gt; &lt; /dev/zero</tt>
        </p>
        <p>
          And received by the command: <tt class="verbatim">nc -l &lt;port&gt; &gt;
          /dev/null</tt>
        </p>
        <p>
          
        </p>
      </li>
      <li>
        <p>
          Usually we sent those zeroes from C0 to ZS. On ZS the number and
          details of incoming traffic was measured using the <tt class="verbatim">tcpdump</tt>
          utility and written to a .pcap file, which we later analysed using
          the software <tt class="verbatim">wireshark</tt>. <tt class="verbatim">wireshark</tt>
          was also used for generating the throughput graphs in this works.
          All .pcap files of all experiments can be found on the disk
          accompanying this document in the subfolder of every experiment.
        </p>
        <p>
          If we changed traffic direction or measurement points in a
          experiment it will be noted explicitly in the evaluation of the
          experiment.
        </p>
      </li>
    </ol>
    <p>
      While most of the properties were chosen for test-setup-practical
      reasons the choice for bulk traffic was done because bulk traffic is
      necessary measure the CPU performance and maximal possible throughput.
    </p>
    <h2 id="auto-87">7.2<span style="margin-left: 1em"></span>Pre Measurements</h2>
    <h3 id="auto-88"><a id="pre-measurements"></a>7.2.1<span style="margin-left: 1em"></span>Pre-Measurement without
    Tunneling Daemon, 1 Link</h3>
    <p>
      A short test of the network connection without any tunneling or vpn
      daemon enabled gave a pure netto throughput rate of about 92.1 MByte/s.
      The CPU Load was about 18% for the nc process (about 80 of this in
      kernel space) on one of the 8 hyper threading cores of MTC.
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -0.55em">
          <tbody><tr>
            <td><table style="display: inline; vertical-align: -1.1em">
              <tbody><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid; border-top: 1px solid"></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Throughput in MByte/s</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid"><p>
                  CPU time percentage of related nc process
                </p><p>
                  (of one virtual hyperthreaded core)
                </p></td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">IG0</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">92.1</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">18</td>
              </tr></tbody>
            </table></td>
          </tr></tbody>
        </table></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Table 7.1. </b><a id="auto-89"></a><a id="pre-measurement-1"></a>Netto Bandwidth and CPU
            Load without Tunneling, 1 Link
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <h3 id="auto-90">7.2.2<span style="margin-left: 1em"></span>Pre-Measurement without Tunneling Daemon,
    2 Links</h3>
    <p>
      Using 2 Ethernet physical links between MTC and MTS (and 2 netcat
      processes one for each interface) the throughput measured was:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -0.55em">
          <tbody><tr>
            <td><table style="display: inline; vertical-align: -2.2em">
              <tbody><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid; border-top: 1px solid"></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Throughput in MByte/s</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid"><p>
                  CPU time percentage of related nc process
                </p><p>
                  (of one virtual hyperthreaded core)
                </p></td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">IG0</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">46.2</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">10.9</td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">IG1</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">46.2</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">10.9</td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">Total</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">92.4</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center"> - </td>
              </tr></tbody>
            </table></td>
          </tr></tbody>
        </table></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Table 7.2. </b><a id="auto-91"></a>Netto Bandwidth and CPU Load without
            Tunneling, 2 Links, measured on MTC
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      Again, about 80% of the CPU time related to nc were spent in kernel
      space. The values on MTS were equivalent. Here you see the throughput
      stays equivalent no matter if we use one virtual VLAN interface (Table
      <a href="#pre-measurement-1">7.1</a> ) or two. Actually using 2 virtual interfaces the
      traffic is a little higher but 0.3 MB/s are in the area of the normal
      throughput fluctuations we observed, the same is true for the CPU time
      percentage. 
    </p>
    <h3 id="auto-92">7.2.3<span style="margin-left: 1em"></span>Pre-Measurement Multipath-vpn, 2 Links,
    without tcpdump</h3>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -0.55em">
          <tbody><tr>
            <td><table style="display: inline; vertical-align: -1.65em">
              <tbody><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid; border-top: 1px solid"></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Throughput in MByte/s</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">CPU Load</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Times: user-mode/kernel-mode</td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">MTC</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; vertical-align: middle">19</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center"><p>
                  <span style="margin-left: 4em"></span>98% 
                </p><p>
                  (only one core used)
                </p></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; vertical-align: middle"><var>&asymp;</var><span style="margin-left: 0.6em"></span>60%/40%</td>
              </tr><tr>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">MTS</td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center"></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center"><p>
                  <span style="margin-left: 3em"></span>98%
                </p><p>
                  (only one core used)
                </p></td>
                <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; vertical-align: middle"><var>&asymp;</var><span style="margin-left: 0.5em"></span>60%/40%</td>
              </tr></tbody>
            </table></td>
          </tr></tbody>
        </table></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Table 7.3. </b><a id="auto-93"></a>Rough Throughput Values of Network
            with Multipath-VPN enabled
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      The fact we already schematically mentioned in section <a href="#vlan-conf">7.1.1.2</a>
      you see here in concrete numbers, fortunately the tunnel throughput of
      19 MB/s is less than half of the pure untunneled throughput 92.1
      MByste/s.
    </p>
    <p>
      
    </p>
    <h2 id="auto-94">7.3<span style="margin-left: 1em"></span>Results on CPU Performance (I)</h2>
    <p>
      The questions to be answered regarding CPU performance are several.
      Therefore this section will answer the following 4 questions, each in a
      own subsection:
    </p>
    <ol>
      <li>
        <p>
          &ldquo;Are the CPU loads on MTC and MTS different, so we have to
          measure them separately in every experiment or is there a
          correlation&rdquo;.
        </p>
      </li>
      <li>
        <p>
          &ldquo;Is there a correlation between throughput and CPU time and if
          yes, is it linear?&rdquo;
        </p>
      </li>
      <li>
        <p>
          What percentage of the CPU load happens in user-space which
          percentage happens in kernel space. Where lies the optimisation
          potential.
        </p>
      </li>
    </ol>
    <p>
      
    </p>
    <p>
      
    </p>
    <h3 id="auto-95">7.3.1<span style="margin-left: 1em"></span>Comparison of CPU Time Usage on MTC and
    on MTS (I)</h3>
    <h4 id="auto-96">7.3.1.1<span style="margin-left: 1em"></span>Goals</h4>
    <p>
      By means of these experiments we wanted to find out whether there is a
      correlation between CPU time usages on MTC on MTS. 
    </p>
    <p>
      From a theoretical point of view there should be a correlation. Every
      package processed on MTC has to be processed on MTS in a comparable way
      and vice versa. Nevertheless this should not be assumed quietly for the
      following experiments there we did these measurements first.
    </p>
    <h4 id="auto-97">7.3.1.2<span style="margin-left: 1em"></span>What we Did</h4>
    <p>
      Our results were measured in a experiment in the following
      configuration:
    </p>
    <ul>
      <li>
        <p>
          The general test setup according to <a href="#general-test-setup">7.1</a>y
        </p>
      </li>
      <li>
        <p>
          Artificial latency created the same way as in the explorative
          experiments in <a href="#Creation_of_artificial_latency">3.1.3.1</a> (using the <tt class="verbatim">tc</tt>
          utility and the <tt class="verbatim">netem</tt> module). Set to 100ms on one
          link and 200ms on the other.
        </p>
      </li>
      <li>
        <p>
          Unidirectional bulk traffic
        </p>
      </li>
      <li>
        <p>
          On MTC and MTS the CPU time usage was measured using the <tt class="verbatim">pidstat</tt>
          utility of the sysstat package (available on all major GNU/Linux
          distributions). The concrete command used was:<span style="margin-left: 1em"></span><tt
          class="verbatim">pidstat -wrud 1 &gt; results</tt>.
        </p>
        <p>
          The full pidstat output results are available on the CD accompanying
          this work
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    7.3. In the Folder:
                    <samp>Experimente/second_experiment_series(07.02.16)/orig/100ms_200ms/with_perf_measurements</samp>s
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-7.3"></a>
          <sup><class style="font-style: normal"><a href="#footnote-7.3">7.3</a></class></sup>
          (or in the github repo belonging to this work, if you're currently
          reading the online version of this document)
        </p>
      </li>
    </ul>
    <p>
      Also we did a second performance measurement experiment with the
      following configuration:
    </p>
    <ul>
      <li>
        <p>
          Similar to the previous first one
        </p>
      </li>
      <li>
        <p>
          But producing no artificial latencies at all.
        </p>
      </li>
    </ul>
    <h4 id="auto-98">7.3.1.3<span style="margin-left: 1em"></span>Results and Interpretation</h4>
    <p>
      Figure <a href="#cpu-time-comparison">7.3</a> shows the CPU time usage on MTC and MTS side by
      side:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-4.png"></img><img src="Bachelorarbeit_english-5.png"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.3. </b><a id="auto-99"></a><a id="cpu-time-comparison"></a>CPU Time Usage over Time
            on MTS and MTC compared Side by Side
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      
    </p>
    <p>
      As you can see the CPU Time usage over time is quite similar quantity
      and quality, every time there is a usage Time breakdown on MTC an
      equivalent one happens on MTC, the same is true for spikes. These
      correlation gets even more visible if you look at the concrete
      percentage values in the full results included in Appendix <a href="#perf-log-appendix">A</a>,
      there the percentage values for a corresponding time entry since
      transfer start often match with little to no difference.
    </p>
    <p>
      The same results were observed for measurements with no artificial delay
      on both links (second experiment), the CPU usage values showed strong
      similarity. Since they were quite redundant no graphs were included for
      them, but the <tt class="verbatim">pidstat</tt> output is included on the CD as
      well.
    </p>
    <p>
      We have to mention that all this CPU usage happened on 1 core, therefore
      all this graphs only contain the load of one core. Multipath VPN has no
      multithreading functionality at all, this will be discussed more
      thoroughly in the next two sections.
    </p>
    <p>
      
    </p>
    <h3 id="auto-100">7.3.2<span style="margin-left: 1em"></span>Correlation between Throughput and CPU
    Time (II)</h3>
    <h4 id="auto-101">7.3.2.1<span style="margin-left: 1em"></span>Goals</h4>
    <p>
      Through these experiments we wanted to find out if there is a connection
      between CPU time usage and throughput.
    </p>
    <h4 id="auto-102">7.3.2.2<span style="margin-left: 1em"></span>What we Did</h4>
    <p>
      Our results were measured in a experiment in the following
      configuration:
    </p>
    <ul>
      <li>
        <p>
          The general test setup according to <a href="#general-test-setup">7.1</a>
        </p>
      </li>
      <li>
        <p>
          No artificial latency
        </p>
      </li>
      <li>
        <p>
          CPU time measurement with <tt class="verbatim">pidstat</tt> exactly like in
          experiment series (I)
        </p>
        <p>
          Again, the full results are available on the disk accompanying this
          document.
        </p>
      </li>
      <li>
        <p>
          Unidirectional bulk traffic
        </p>
      </li>
    </ul>
    <h4 id="auto-103">7.3.2.3<span style="margin-left: 1em"></span>Results and Interpretation</h4>
    <p>
      A side by side comparison of CPU time usage on MTS and the throughput
      received at ZS is visible in figure <a href="#trafic-cpu-comarison">7.4</a>.
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-6.png"></img> <img src="Bachelorarbeit_english-7.png" width="400"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.4. </b><a id="auto-104"></a><a id="trafic-cpu-comarison"></a>Side by Side comparison
            of CPU Time Usage on Tunnel Exit (MTS) and Data Throughput
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      While the two graphs are not completely equal they are quite similar.
      Every time the throughput rate drops (most likely because of reordering
      induced send window reductions), the CPU time usage drops in a similar
      fashion. For example this can be observed at second 7, 14, 29, 41, 51
      and finally at the transmission termination at second 60.
    </p>
    <p>
      After this results we can safely assume that throughput and CPU time
      usage are directly linked in Multipath VPN. Especially since this
      findings match with our understanding of the code and the way Multipath
      VPN works. Every packet has to be processed separately and this
      processing of course costs CPU time.
    </p>
    <h3 id="auto-105">7.3.3<span style="margin-left: 1em"></span>Differentiated CPU Usage Analysis
    (III)</h3>
    <h4 id="auto-106">7.3.3.1<span style="margin-left: 1em"></span>Goals</h4>
    <p>
      By means of these experiments we wanted to find out where exactly all
      the CPU time is used and for what. Since the used measurement tool <tt
      class="verbatim">pidstat</tt> shows <em>system-time</em> and <em>user-time</em>
      separately, we can get some insights on this question.
    </p>
    <h4 id="auto-107">7.3.3.2<span style="margin-left: 1em"></span>What we Did</h4>
    <p>
      Our results were measured in a experiment in the following
      configuration:
    </p>
    <ul>
      <li>
        <p>
          The general test setup according to <a href="#general-test-setup">7.1</a>
        </p>
      </li>
      <li>
        <p>
          No artificial latency
        </p>
      </li>
      <li>
        <p>
          CPU time measurement with <tt class="verbatim">pidstat</tt> exactly like in
          experiment series (I)
        </p>
        <p>
          Again, the full results are available on the disk accompanying this
          document.
        </p>
      </li>
      <li>
        <p>
          Unidirectional bulk traffic
        </p>
      </li>
    </ul>
    <h4 id="auto-108">7.3.3.3<span style="margin-left: 1em"></span>Results and Interpretation</h4>
    <p>
      Since we are not interested in the changing of the <em>system-time</em>
      to <em>user-time</em> ratio over time but the average values throughout
      the whole experiments the results are displayed in table <a href="#time-ratio">7.4</a>:
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -1.65em">
          <tbody><tr>
            <td style="border-right: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-top: 1px solid">Average <em>user-cpu-time</em> share</td>
            <td style="border-right: 1px solid; border-bottom: 1px solid; border-top: 1px solid">56.4833 %</td>
          </tr><tr>
            <td style="border-right: 1px solid; border-bottom: 1px solid; border-left: 1px solid">Average <em>system-cpu-time</em> share </td>
            <td style="border-right: 1px solid; border-bottom: 1px solid">39.4666 %</td>
          </tr><tr>
            <td style="border-right: 1px solid; border-bottom: 1px solid; border-left: 1px solid"><em>user-time/system-time</em> ratio</td>
            <td style="border-right: 1px solid; border-bottom: 1px solid">1.431</td>
          </tr></tbody>
        </table></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Table 7.4. </b><a id="auto-109"></a><a id="time-ratio"></a>Percentages of User and
            System Time of the Multipath VPN Perl Process in a Network with no
            Artificial Latency and 100% cpu load of one core.
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      We were expecting that Multipath VPN would spent most of it's time in
      kernel space (i.e. system time) since the context switches necessary
      between user-mode (taking the packets from tun interface) to system-mode
      (processing and sending the packets through the network interfaces), are
      usually a quite time consuming task[<a href="#bib-rosen2013linux">Ros13</a>] and are
      accounted as system time[<a href="#bib-benvenuti2006">Ben06</a>][<a href="#bib-rosen2013linux">Ros13</a>].
    </p>
    <p>
      This is a strong indicator that in userspace the process is doing some
      extremely expensive and perhaps (non necessary) work. This could be
      frequent copying of data between functions or processing or reading the
      data in some way like for example with a regular expression engine (This
      suspicion is mentioned because using regular expressions on data is a
      very common way often used to solve problems in Perl).
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <p>
      
    </p>
    <h2 id="auto-110">7.4<span style="margin-left: 1em"></span>Results on Connection Stability (II)</h2>
    <h3 id="auto-111">7.4.1<span style="margin-left: 1em"></span>Goals</h3>
    <p>
      For this section we wanted to stress test the connection stability
      features of Multipath VPN. Or in other words: How changes the throughput
      over time if one of the links between MTC and MTS becomes unavailable?
    </p>
    <h3 id="auto-112">7.4.2<span style="margin-left: 1em"></span>What we did</h3>
    <p>
      Our results were measured in a experiment in the following
      configuration:
    </p>
    <ul>
      <li>
        <p>
          The general test setup according to <a href="#general-test-setup">7.1</a>
        </p>
      </li>
      <li>
        <p>
          No artificial latency at all
        </p>
      </li>
      <li>
        <p>
          At second 45 we took the virtual network interface eth2.5 on MTS
          down and with this disconnected the link IG0 to MTC. 20 seconds
          later we took eth2.5 up again and therefore reconnected.
          <p>
            <font size="-1"><div align="justify">
              <div style="margin-left: 0px">
                <div style="margin-right: 0px">
                  <class style="font-style: normal"><p>
                    7.4. Since these timings and manipulations were done
                    manually and using by looking on a stopwatch, perfect
                    accuracy can not be guaranteed. It's also possible the
                    real interrupt time were 19 or 22 seconds, therefore the
                    results of these experiments have to be taken with a grain
                    of salt and are rather roundabout qualitative values.
                  </p></class>
                </div>
              </div>
            </div></font>
          </p>
          <span style="margin-left: 0em"></span>
          <a id="footnr-7.4"></a>
          <sup><class style="font-style: normal"><a href="#footnote-7.4">7.4</a></class></sup>
        </p>
      </li>
      <li>
        <p>
          Traffic sent from CO to ZS and measured via tcpdump on ZS. (Same as
          in general setup)
        </p>
      </li>
    </ul>
    <h3 id="auto-113">7.4.3<span style="margin-left: 1em"></span>Results</h3>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Anhang_CD_Daten/Experimente/second_experiment_series(07.02.16)/plug_out_perf/opt_0ms,0ms/zs_traffic_C0-&gt;ZS.pdf" width="700"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.5. </b><a id="auto-114"></a>Throughput over time when
            disconnection one uplink for 20 seconds
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      
    </p>
    <p>
      
    </p>
    <h3 id="auto-115">7.4.4<span style="margin-left: 1em"></span>Evaluation and Interpretation</h3>
    <p>
      Here you can observe several things:
    </p>
    <ul>
      <li>
        <p>
          After we disconnected one link the throughput over Multipath VPN
          suddenly dropped to zero Bytes/s and stagnated completely for about
          7 seconds. It took Multipath VPN about 8 seconds to recover
          completely.
        </p>
        <p>
          <strong>Explanation:</strong>
        </p>
        <p>
          We currently assume that this drop to zero happens as a result of
          the following process: 
        </p>
        <ul>
          <li>
            <p>
              In the code of Multipath VPN there exists no feature to get the
              state of the used network interface or to be informed of the
              unavailability of an interface.
            </p>
          </li>
          <li>
            <p>
              So short-time-speaking Multipath VPN does not know about a link
              loss and continues to put packets into the socket belonging to
              the broken link.
            </p>
          </li>
          <li>
            <p>
              Since all this packets won't be delivered this means a huge
              packet loss.
            </p>
          </li>
          <li>
            <p>
              This packet loss causes the sender (C0) to reduce his send
              window (cwnd) further and further. 
            </p>
          </li>
          <li>
            <p>
              And during this time no stream traffic will can be transported
              because only every second packet arrives.
            </p>
          </li>
          <li>
            <p>
              There exists a Target Reach-ability Check loop, but it runs all
              5 seconds. So eventually this loop notices that the link is down
              and disable the corresponding
            </p>
          </li>
          <li>
            <p>
              Now a continuous packet stream is possible again and TCP can
              restore it's original throughput and send window size (cwnd).
            </p>
          </li>
        </ul>
        <p>
          The Target Reach-ability Check loop interval of 5 seconds fits quite
          well to the observed values.
        </p>
      </li>
      <li>
        <p>
          Nevertheless the TCP Connection did not break or disconnect.
        </p>
      </li>
      <li>
        <p>
          After recovering the throughput returned to its old level, although
          using only one link.
        </p>
        <p>
          <strong>Explanation:</strong>
        </p>
        <p>
          In this experiment, without any artificial latency, the throughput
          bottleneck is not the network but the cpu. Even one Ethernet wire
          can transport the maximal CPU processable throughput of 20 MB/s.
        </p>
      </li>
      <li>
        <p>
          It even seems the throughput is somewhat increased using one link
          and decreases a little when the link is reconnected again.
        </p>
        <p>
          <strong>Explanation:</strong>
        </p>
        <p>
          This can have 2 reasons:
        </p>
        <ol>
          <li>
            <p>
              A lower amount of packet reordering means lesser spuriously
              assumed packet losses and send window reductions.
            </p>
          </li>
          <li>
            <p>
              The existence of only one link means less computational work is
              necessary for Multipath VPN. The complete &ldquo;decision
              making&rdquo; process which had to be done for every packet gets
              quite simple now, perhaps this reduced cpu power need is the
              reason for the slightly increased throughput.
            </p>
          </li>
        </ol>
        <p>
          <span style="margin-left: 2em"></span>In the end, the time span in which the increased
          traffic occurred was to short to decide which one of this reasons is
          responsible. Also the increase in throughput is not that significant
          to decide anything for sure. It is up to future studies or
          experiments to analyse this phenomenon more precisely.
        </p>
      </li>
    </ul>
    <p>
      This experiment though not very sophisticated and without any additional
      measurement besides traffic, gave several deep insights into the
      workings und logic of Multipath VPN.
    </p>
    <p>
      
    </p>
    <h2 id="auto-116"><a id="linux-tcp-options"></a>7.5<span style="margin-left: 1em"></span>Results of Changing Linux
    Internal TCP Options (III)</h2>
    <p>
      In section <a href="#tcp-switches-def">4.2.1</a> we introduced two Linux internal TCP
      configuration switches which can improve the throughput performance in a
      network suffering of reordering. With these experiments we will evaluate
      how much throughput or traffic regularity can be approved with several
      different configurations.
    </p>
    <h3 id="auto-117">7.5.1<span style="margin-left: 1em"></span>Goals</h3>
    <p>
      In this section we want to find out what effect changing the Linux tcp
      options <tt class="verbatim">tcp_reordering</tt>  on the sending computer and on
      sender and receiver to different values higher than 3, has on throughput
      through an OSI Layer 3 Link Aggregation Network. 
    </p>
    <p>
      Additionally we will try disabling <tt class="verbatim">tcp_fack</tt> and observe
      the effects.
    </p>
    <h3 id="auto-118">7.5.2<span style="margin-left: 1em"></span>What we did</h3>
    <p>
      Our results were measured in a experiment in the following
      configuration:
    </p>
    <ul>
      <li>
        <p>
          The general test setup according to <a href="#general-test-setup">7.1</a>
        </p>
      </li>
      <li>
        <p>
          No artificial latency at all
        </p>
      </li>
    </ul>
    <p>
      Additionally to this base configuration we carried out several
      experiments in two different series:
    </p>
    <h4 id="auto-119">7.5.2.1<span style="margin-left: 1em"></span>Series 1: Only Changing Sender TCP
    Configuration</h4>
    <p>
      On the Sender (C0) the configurations were:
    </p>
    <ul>
      <li>
        <p>
          tcp_reordering = 5
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 10
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 15
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 15 + tcp_fack disabled
        </p>
      </li>
    </ul>
    <h4 id="auto-120">7.5.2.2<span style="margin-left: 1em"></span>Series 2: Changing Sender and Receiver
    Configuration</h4>
    <p>
      On both computers the configurations were:
    </p>
    <ul>
      <li>
        <p>
          tcp_reordering = 15 + tcp_fack disabled
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 30 + tcp_fack disabled
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 300 + tcp_fack disabled
        </p>
      </li>
      <li>
        <p>
          tcp_reordering = 0 + tcp_fack disabled
        </p>
      </li>
    </ul>
    <p>
      
    </p>
    <h3 id="auto-121">7.5.3<span style="margin-left: 1em"></span>Results and Interpretation</h3>
    <p>
      Frustratingly adjusting the mentioned options does not change very much
      at all in means of throughput. This subsection contains 3 throughput
      over time figures, the first one is measured with the default settings
      for these 2 options, in the second one we changed <tt class="verbatim">tcp_reordering</tt>
      to 15 and disables <tt class="verbatim">tcp_fack</tt> on sender site, for the
      third one set this values for sender and receiver site:
    </p>
    <p>
      
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-8.png" width="650"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.6. </b><a id="auto-122"></a>Network Throughput Diagram, Without
            any TCP Options
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-9.png" width="650"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.7. </b><a id="auto-123"></a>Network Throughput Diagram, with
            Sender side modifications: tcp_reordering=15 and tcp_fack disabled
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      
    </p>
    <table style="width: 100%">
      <tbody><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em"><img src="Bachelorarbeit_english-10.png" width="650"></img></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
      </tr><tr>
        <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
          <font size="-1"><p>
            <b>Figure 7.8. </b><a id="auto-124"></a>Network Throughput Diagram, with
            Sender and Receiver-side modifications: tcp_reordering=15 and
            tcp_fack disabled
          </p></font>
        </p></td>
      </tr></tbody>
    </table>
    <p>
      
    </p>
    <p>
      None of the three figures displays a smooth transfer rate, all 3 contain
      oscillations of comparable amplitude. We assume all this oscillations
      are a result of tcp performing fast recovery after receiving several
      reordered packets.
    </p>
    <p>
      Another possibility to compare these three scenarios is by total
      throughput, table <a href="#total-throughput">7.5</a> contains the total throughput of all
      3 experiments, measured by the last Acknowledgement number of the last
      ACK segment sent by the receiver.
    </p>
    <p>
      <table style="width: 100%">
        <tbody><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em"><table style="display: inline; vertical-align: -0.55em">
            <tbody><tr>
              <td></td>
            </tr></tbody>
          </table><table style="display: inline; vertical-align: -2.2em">
            <tbody><tr>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid; border-top: 1px solid">Experiment</td>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-top: 1px solid">Total Throughput</td>
            </tr><tr>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid">No Options set</td>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">1105.945953 MByte</td>
            </tr><tr>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid"><tt class="verbatim">tcp_reorder</tt>=15 and <tt class="verbatim">tcp_fack</tt>
              disabled on sender</td>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">1091.303905 MByte</td>
            </tr><tr>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center; border-left: 1px solid"><tt class="verbatim">tcp_reorder</tt>=15 and <tt class="verbatim">tcp_fack</tt>
              disabled on both hosts</td>
              <td style="border-right: 1px solid; border-bottom: 1px solid; text-align: center">1096.380697 MByte</td>
            </tr></tbody>
          </table></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; height: 0.5em"></td>
        </tr><tr>
          <td style="text-align: center; padding-left: 0em; padding-right: 0em; padding-left: 1.5em; padding-right: 1.5em"><p>
            <font size="-1"><p>
              <b>Table 7.5. </b><a id="auto-125"></a><a id="total-throughput"></a>Total Throughput in the
              three TCP Options experiments, compared
            </p></font>
          </p></td>
        </tr></tbody>
      </table>
      
    </p>
    <p>
      Again the values don't vary very much, there doesn't seem to be a big
      difference.
    </p>
    <h4 id="auto-126">7.5.3.1<span style="margin-left: 1em"></span>Explanation</h4>
    <p>
      Since the two TCP options used are sparely documented kernel internals
      currently we can only make assumptions by reading the provided
      documentation. For <tt class="verbatim">tcp_reordering</tt> reading the official
      documentation very precisely word by word can give an answer:
    </p>
    <p style="margin-top: 0.5em; margin-bottom: 0.5em">
      <div style="margin-left: 70.291740657624px">
        <div style="margin-right: 70.291740657624px">
          <div style="text-indent: 0em">
            <p>
              The maximum a packet can be reordered in a TCP packet stream
              without TCP assuming packet loss and going into slow start.
            </p>
          </div>
        </div>
      </div>
    </p>
    <p>
      Here the term is &ldquo;and going into slow start&rdquo; which means the
      sender starts again with a cwnd window of 1. It does not say anything
      about preventing halving or reducing cwnd, so we assume this option
      would have solved a problem we never had(an even more critical
      processing of DUPACKs) but not our problem.
    </p>
    <p>
      
    </p>
    <p>
      Similar would be a are a explanation for the lacking efficacy of <tt
      class="verbatim">tcp_fack</tt>. As a reminder <tt class="verbatim">tcp_fack</tt> works
      together with <tt class="verbatim">tcp_sack</tt>, the <em>Selective
      Acknowledgement</em> extension of TCP. With <tt class="verbatim">tcp_sack</tt> a
      receiver can tell the sender &ldquo;I received packet 7,9 and 13&rdquo;,
      usually the packets in between are assumed as in flight, and measures
      are only taken if DUPACKs or Timeouts occurs. The change <tt class="verbatim">tcp_fack</tt>
      introduces that such &ldquo;gap packets&rdquo; are explicit assumed
      lost, resent and cwnd is adjusted. 
    </p>
    <p>
      Such &ldquo;gap packets&rdquo; and corresponding of SACK messages often
      occur in networks suffering from a high rate of reordering, where the
      packets are not really lost, in these cases <tt class="verbatim">tcp_fack</tt>
      assumes packets as lost which aren't and overcorrects to aggressively,
      so disabling <tt class="verbatim">tcp_fack</tt> can bring advantages.
      Unfortunately in our case the reason for the frequent cwnd adaptions
      doesn't seem to be SACK and FACK induced retransmission and cwnd
      reducing, so disabling <tt class="verbatim">tcp_fack</tt> showed no big effect.
    </p>
    <h1 id="auto-127">Chapter 8<br></br>Conclusion</h1>
    <h2 id="auto-128">8.1<span style="margin-left: 1em"></span>Evaluation Summary and Optimisation
    Possibilities</h2>
    <p>
      Finally we can conclude that the OSI Layer 3 Link Aggregation Tunneling
      implementation evaluated <tt class="verbatim">Multipath VPN</tt> is still in it's
      child shoes. It works but throughput, cpu time need and reliability
      behaviour have issues. For most of these issues fixes are possible which
      are summarised:
    </p>
    <dl>
      <p>
        <dt>
          CPU Time Need
        </dt>
        <dd>
          <p>
            Through several experiments we detected that the CPU time need of
            Multipath VPN is quite high. On a Intel Core i7-2600 the maximum
            reachable throughput was 20 MB/s. Depending on the capability of
            the internet uplinks and latencies this can be a boundary.
          </p>
          <p>
            <strong>Possibilities for Optimization:</strong>
          </p>
          <p>
            Fortunately several possibilities for optimisation exist:
          </p>
          <dl>
            <p>
              <dt>
                Multithreading
              </dt>
              <dd>
                <p>
                  Since Multipath VPN is single threaded only one core of the
                  8 ones in the experiment was used. Here Multithreading would
                  help. One way to distribute the workload would be to assign
                  one thread for the &ldquo;receiving direction&rdquo; and one
                  core for the &ldquo;sending direction&rdquo;.
                </p>
                <p>
                  All further splitting salvages the possibility of locking
                  problems or creation of additional packet reordering. To
                  avoid additional reordering every thread could handle one
                  TCP connection. Nevertheless without further research we can
                  not safely tell how well such further splitting to more than
                  2 threads perform since additional locking for sockets and
                  the tun interface may be necessary with an correlative
                  performance penalty.
                </p>
              </dd>
            </p>
            <p>
              <dt>
                Algorithmic Optimizations
              </dt>
              <dd>
                <p>
                  In all the experiments we measured CPU time the amount of
                  CPU time spent in user space was larger than that spent in
                  kernel space. In the least extreme case the ratio of
                  <em>user-time</em> to <em>system-time</em> was 1.4. This is
                  strange for a program that does a lot of context switches,
                  since these are expensive and are accounted  as
                  <em>system-time</em>. 
                </p>
                <p>
                  This high ratio of <em>user-time</em> is a strong indicator
                  for available optimisation potential. And Fortunately
                  optimisation of user space code is way less complicated than
                  optimising kernel space and getting the changes merged into
                  the upstream project.
                </p>
              </dd>
            </p>
            <p>
              <dt>
                Kernel Space Reeimplementation
              </dt>
              <dd>
                <p>
                  This of course would be the most drastic and costly measure.
                  And the most effective, in kernel space all data copying
                  between kernel space and user space would vanish as well as
                  all privilege mode switches.
                </p>
              </dd>
            </p>
          </dl>
        </dd>
      </p>
      <p>
        <dt>
          Reliability Behaviour
        </dt>
        <dd>
          <p>
            If one of several links goes down the traffic tunneled through
            Multipath VPN stagnates to zero. After five seconds Multipath VPN
            realizes the unavailability of this links and adapts its
            distribution of packets to links, and the packet flow regenerates.
            The 5 second interval is hardcoded. While tcp connections survive
            this stagnation for voip sessions over UDP this stagnation is
            problematic.
          </p>
          <p>
            <strong>Possibilities for Optimization:</strong>
          </p>
          <p>
            On most modern Linux systems it is possible to register for the
            event notification of connection loss and so to get informed of
            every connection loss via DBUS <strong>immediately</strong>. This
            would allow a much quicker handling of connection loses, a
            smoother drop to the new throughput rate and therefore a much less
            disturbing users experience.
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Throughput and Reordering
        </dt>
        <dd>
          <p>
            Our experiments showed that a packet stream being tunneled through
            Multipath VPN gets reordered, especially heavy in networks with
            low latencies. This reordering is noxious for the way TCP tries to
            detect packets losses and triggers spurious retransmit and send
            rate reductions. Although this effect never was the limiting
            factor for the maximum possible throughput (CPU power, or latency
            and Receive Window size was), it was the reason for serious
            staggering of the throughput rate. In one experiment, the
            reordering induced a number of Duplicate Acknowledgements that
            high, the sender stopped to send at all for about 0.4 seconds.
          </p>
          <p>
            <strong>Possibilities for Optimization:</strong>
          </p>
          <p>
            The source of packet reordering is not Multipath VPN or different
            latencies, but the way the Linux kernel buffers and processes
            packets. On Linux it is not possible to send a UDP packet
            <strong>immediately</strong> to the wire, forcefully pushing it
            through the ring buffer of the network device. Packets get
            buffered in this device ring buffer and sent together in a way a
            user space process has no influence on. So if the avoidance of the
            OS buffering induced reordering is impossible, what can we do?
          </p>
          <p>
            Basically there exist two options:
          </p>
          <dl>
            <p>
              <dt>
                Re-reordering on Both sites
              </dt>
              <dd>
                <p>
                  It is possible to create a buffer of the unpacked packets
                  (one buffer for each TCP connection) and re-reorder an array
                  of pointers to the segments according to their sequence
                  numbers. While this solves the reordering issue it
                  introduces two new problems: additional cpu time need and a
                  higher latency for every packet, since it will stay in this
                  re-reordering buffer for some time.
                </p>
              </dd>
            </p>
            <p>
              <dt>
                Making TCP Resistant to Packet Reordering
              </dt>
              <dd>
                <p>
                  Linux TCP offers two switches accessible via the <tt class="verbatim">/proc/sys/net</tt>
                  virtual file system which can make TCP more resistant to
                  reordering and avoid the possibilities of spurious
                  retransmit and send rate reduction.  In our experiments we
                  evaluated two of them but could not measure any change in
                  throughput stability.
                </p>
              </dd>
            </p>
          </dl>
        </dd>
      </p>
    </dl>
    <h2 id="auto-129">8.2<span style="margin-left: 1em"></span>About the Application Domain</h2>
    <p>
      Following the tradition of TCP/IP Illustrated by Richard Stevens[<a
      href="#bib-Stevens:1993:TIP">Ste93</a>] we distinguish between two different uses of transport
      layer protocols:
    </p>
    <ol>
      <li>
        <p>
          Interactive Data Flow
        </p>
        <p>
          Examples for this category of traffic are:
        </p>
        <ul>
          <li>
            <p>
              Remote shell or remote desktop sessions (like ssh or windows
              rdp)
            </p>
          </li>
          <li>
            <p>
              Remote Procedure Calls (RPC)
            </p>
          </li>
          <li>
            <p>
              Online Gaming (Quake, OpenArena)
            </p>
          </li>
        </ul>
        <p>
          <span style="margin-left: 1em"></span>For a good interactive data flow service a low
          network latency is very important, a high network throughput is no
          core requirement.
        </p>
      </li>
      <li>
        <p>
          Bulk Data Flow
        </p>
        <p>
          Examples for this category of traffic are:
        </p>
        <ul>
          <li>
            <p>
              Downloads
            </p>
          </li>
          <li>
            <p>
              Movie Streaming
            </p>
          </li>
          <li>
            <p>
              Uploads
            </p>
            <p>
              The requirements on the network of this category are the
              opposite of the previous category. For bulk data flow
              applications a high throughput is very important to work
              satisfactory whereas low latency is secondary.
            </p>
          </li>
        </ul>
      </li>
    </ol>
    <p>
      Knowing these examples it gets clear that bulk data flow is the
      preferred application domain of OSI Layer 3 Link Aggregation Tunneling
      solutions, since in this domain it's advantages, higher throughput and
      reliability are needed. Of course the additional packet processing means
      a small additional latency. This small additional latency means no
      significant problem for interactive data flow. Nevertheless compared to
      load balancing it makes OSI Layer 3 Link Aggregation Tunneling less
      optimal for users only needing interactive data flow.
    </p>
    <h1 id="auto-130"><a id="perf-log-appendix"></a>Appendix A<br></br>Detailed Performance Measurement
    Results</h1>
    <h2 id="auto-131">A.1<span style="margin-left: 1em"></span>MTS: 100ms_200ms latency</h2>
    <p>
      <span style="margin-left: 1em"></span>
    </p>
    <pre class="verbatim" xml:space="preserve">
#Time      %usr %system   %CPU   CPU     cswch/s nvcswch/s  Command
01001      1.00    0.00   1.00     0        7.00      0.00  perl
01002      0.00    0.00   0.00     0       15.00      0.00  perl
01003     10.00    5.00  15.00     0       99.00     15.00  perl
01004     25.00   12.00  37.00     0      724.00     25.00  perl
01005     28.00   12.00  40.00     0     1276.00     24.00  perl
01006     27.00   14.00  41.00     0     1087.00     29.00  perl
01007     25.00   13.00  38.00     0      950.00     26.00  perl
01008     25.00   13.00  38.00     0      965.00     24.00  perl
01009     23.00    6.00  29.00     4      415.00     12.00  perl
01010     21.00   14.00  35.00     0      456.00     22.00  perl
01011     22.00   13.00  35.00     0      528.00     25.00  perl
01012     29.00   16.00  45.00     0     1075.00     36.00  perl
01013     30.00   11.00  41.00     0     1011.00     24.00  perl
01014     31.00   14.00  45.00     0     1044.00     29.00  perl
01015     30.00   12.00  42.00     0      859.00     19.00  perl
01016     35.00   13.00  48.00     1     1264.00     25.00  perl
01017     21.00   11.00  32.00     0      551.00     28.00  perl
01018     26.00   14.00  40.00     0      923.00     27.00  perl
01019     31.00   17.00  48.00     0     1191.00     27.00  perl
01020     35.00   12.00  47.00     4     1396.00     19.00  perl
01021     35.00   12.00  47.00     0     1341.00     28.00  perl
01022     32.00   14.00  46.00     0     1696.00     17.00  perl
01023     35.00   15.00  50.00     0     1503.00     31.00  perl
01024     33.00   14.00  47.00     1     1561.00     21.00  perl
01025     38.00    7.00  45.00     1     1599.00     14.00  perl
01026     33.00   13.00  46.00     0     1265.00     20.00  perl
01027     38.00   13.00  51.00     0     1241.00     31.00  perl
01028     40.00   15.00  55.00     1     1574.00     20.00  perl
01029     39.00   15.00  54.00     1     1651.00     20.00  perl
01030     41.00   17.00  58.00     0     1453.00     30.00  perl
01031     29.00   16.00  45.00     0      813.00     38.00  perl
01032     33.00   20.00  53.00     0      991.00     37.00  perl
01033     36.00   19.00  55.00     0     1448.00     39.00  perl
01034     40.00   21.00  61.00     0     1196.00     37.00  perl
01035     41.00   21.00  62.00     0     1188.00     39.00  perl
01036     45.00   15.00  60.00     1     1662.00     24.00  perl
01037     41.00   20.00  61.00     0     1836.00     30.00  perl
01038     46.00   20.00  66.00     3     1857.00     37.00  perl
01039     44.00   18.00  62.00     0     2000.00     36.00  perl
01040     52.00   12.00  64.00     0     1929.00     22.00  perl
01041     42.00   20.00  62.00     0     2035.00     31.00  perl
01042     43.00   15.00  58.00     0     2173.00     37.00  perl
01043     32.00   15.00  47.00     0     1182.00     29.00  perl
01044     28.00   17.00  45.00     0     1316.00     35.00  perl
01045     34.00   11.00  45.00     1     1638.00     15.00  perl
01046     34.00   10.00  44.00     0     1236.00     16.00  perl
01047     30.00    9.00  39.00     0      892.00     19.00  perl
01048     32.00   17.00  49.00     1     1156.00     36.00  perl
01049     32.00   17.00  49.00     0     1490.00     29.00  perl
01050     33.00   13.00  46.00     0     1608.00     30.00  perl
01051     30.00   15.00  45.00     0     1440.00     31.00  perl
01052     23.00   10.00  33.00     0      621.00     23.00  perl
01053     23.00   13.00  36.00     0      854.00     25.00  perl
01054     28.00    7.00  35.00     0      849.00     18.00  perl
01055     25.00   12.00  37.00     0      808.00     26.00  perl
01056     23.00   11.00  34.00     0      735.00     22.00  perl
01057     30.00    8.00  38.00     0      845.00     17.00  perl
01058     29.00   11.00  40.00     0     1266.00     17.00  perl
01059     25.00   16.00  41.00     0     1130.00     27.00  perl
01060     28.00   13.00  41.00     1     1106.00     23.00  perl
01061     31.00    6.00  37.00     2     1025.00      8.00  perl
01062     19.00    4.00  23.00     2      624.00      9.00  perl
01063      0.00    0.00   0.00     0        7.00      0.00  perl
01064      0.00    0.00   0.00     0        7.00      0.00  perl</pre>
    <h2 id="auto-132">A.2<span style="margin-left: 1em"></span>MTC: 100ms_200ms latency</h2>
    <pre class="verbatim" xml:space="preserve">
   #Time     %usr %system     %CPU   CPU     cswch/s nvcswch/s  Command
97360     0.00    0.00     0.00     0        3.00      4.00  perl
97361     0.00    0.00     0.00     0       10.00      4.00  perl
97362     9.00    6.00    15.00     0      122.00     13.00  perl
97363    25.00   11.00    36.00     0      769.00     28.00  perl
97364    28.00    8.00    36.00     0     1216.00     22.00  perl
97365    26.00   13.00    39.00     0     1101.00     22.00  perl
97366    25.00   12.00    37.00     0      904.00     25.00  perl
97367    30.00   10.00    40.00     0      884.00     21.00  perl
97368    21.00    8.00    29.00     0      424.00     19.00  perl
97369    23.00    9.00    32.00     0      525.00     22.00  perl
97370    24.00   13.00    37.00     0      557.00     24.00  perl
97371    28.00   14.00    42.00     0     1157.00     26.00  perl
97372    25.00   13.00    38.00     0     1095.00     28.00  perl
97373    29.00   12.00    41.00     1     1260.00     22.00  perl
97374    23.00   15.00    38.00     0     1035.00     30.00  perl
97375    37.00    9.00    46.00     0     1296.00     14.00  perl
97376    22.00    9.00    31.00     0      619.00     23.00  perl
97377    29.00   12.00    41.00     0      888.00     20.00  perl
97378    32.00   13.00    45.00     0     1147.00     23.00  perl
97379    30.00   16.00    46.00     0     1229.00     27.00  perl
97380    34.00   11.00    45.00     0     1328.00     25.00  perl
97381    33.00   11.00    44.00     0     1494.00     23.00  perl
97382    31.00   14.00    45.00     0     1595.00     22.00  perl
97383    29.00   14.00    43.00     0     1485.00     24.00  perl
97384    29.00   16.00    45.00     0     1374.00     29.00  perl
97385    28.00   18.00    46.00     0     1111.00     32.00  perl
97386    35.00   12.00    47.00     0     1369.00     27.00  perl
97387    36.00   15.00    51.00     0     1546.00     28.00  perl
97388    31.00   19.00    50.00     2     1697.00     24.00  perl
97389    38.00   16.00    54.00     0     1583.00     26.00  perl
97390    30.00   13.00    43.00     0     1046.00     23.00  perl
97391    31.00   18.00    49.00     0     1183.00     41.00  perl
97392    37.00   17.00    54.00     0     1461.00     32.00  perl
97393    39.00   16.00    55.00     0     1526.00     30.00  perl
97394    40.00   16.00    56.00     0     1533.00     31.00  perl
97395    41.00   13.00    54.00     4     1692.00     27.00  perl
97396    43.00   12.00    55.00     0     2033.00     19.00  perl
97397    43.00   17.00    60.00     0     1989.00     33.00  perl
97398    39.00   20.00    59.00     0     2010.00     38.00  perl
97399    37.00   22.00    59.00     0     2061.00     42.00  perl
97400    43.00   16.00    59.00     0     2006.00     34.00  perl
97401    43.00   13.00    56.00     0     2031.00     25.00  perl
97402    32.00   13.00    45.00     0     1407.00     23.00  perl
97403    27.00   15.00    42.00     0     1236.00     30.00  perl
97404    34.00   12.00    46.00     0     1245.00     26.00  perl
97405    29.00   15.00    44.00     0      969.00     39.00  perl
97406    30.00   12.00    42.00     0      791.00     21.00  perl
97407    30.00   15.00    45.00     0     1250.00     31.00  perl
97408    31.00   15.00    46.00     0     1471.00     34.00  perl
97409    32.00   18.00    50.00     0     1391.00     36.00  perl
97410    31.00   13.00    44.00     0     1346.00     24.00  perl
97411    22.00   11.00    33.00     0      586.00     25.00  perl
97412    29.00   10.00    39.00     0      786.00     18.00  perl
97413    22.00   13.00    35.00     0      757.00     20.00  perl
97414    25.00   11.00    36.00     0      827.00     21.00  perl
97415    25.00   10.00    35.00     0      685.00     24.00  perl
97416    26.00   12.00    38.00     0      841.00     24.00  perl
97417    28.00   12.00    40.00     0     1117.00     22.00  perl
97418    27.00   14.00    41.00     0     1001.00     23.00  perl
97419    28.00   11.00    39.00     0     1007.00     22.00  perl
97420    23.00   15.00    38.00     0      820.00     30.00  perl
97421    15.00   10.00    25.00     0      533.00     18.00  perl
97422     0.00    0.00     0.00     0        3.00      4.00  perl
97423     0.00    0.00     0.00     0        3.00      4.00  perl

</pre>
    <h1 id="auto-133">Appendix B<br></br>Perl POE Framework Example</h1>
    <div class="ornament" style=";display:block;">
      <p>
        <p>
          <span class="ornament" style="background-color:white"><font color="white"><b><font color="green">Example <class
          style="font-style: normal">B.1</class></font></b></font></span> Sessions and the Kernel
          in Perl POE
        </p>
        <p>
          The following example is a very short session (and almost unchanged
          part of the Multipath VPN code). It knows 3 events: 
        </p>
        <ol>
          <li>
            <p>
              <strong>_start</strong><span style="margin-left: 1em"></span>(the initialisation
              event)
            </p>
          </li>
          <li>
            <p>
              <strong>partial_init_step_completed</strong><span style="margin-left: 1em"></span>(gets
              &ldquo;called&rdquo; when an initialisation step is completed)
            </p>
          </li>
          <li>
            <p>
              <strong>harakiri</strong><span style="margin-left: 1em"></span>(de-initialise the
              session)
            </p>
          </li>
        </ol>
        <p>
          The purpose of this Session is to notify the outside world, when
          Multipath VPN has set up all necessary networking resources and is
          working. For this 2 partial_init_step_completed events have to be
          dispatched, then a systemd-notification via the <tt class="verbatim">systemd-notify</tt>
          shell utility will be executed. But now lets look at the Code:
        </p>
        <div style="margin-left: 35.145870328812px">
          <div style="text-indent: 0em">
            <div class="compact-block">
              <p>
                <tt class="verbatim"><font color="blue"># Sessions are created
                via:</font></tt>
              </p>
            </div>
          </div>
        </div>
        <div style="margin-left: 35.145870328812px">
          <div style="text-indent: 0em">
            <div class="compact-block">
              <pre class="verbatim" xml:space="preserve">
POE::Session-&gt;create(
    # Events are called inline_states in POE
    inline_states =&gt; {
        <font color="blue"># the _start event and its handler subroutine</font>
        <font color="blue"># sub returns an anonymous function which is the event handler here.</font>
        _start =&gt; sub {
            # store a reference to this session in the global variable
            $systemd_event_session = $_[SESSION];
        },
        <font color="blue"># the partial_init_step_completed event and its handler.</font>
        partial_init_step_completed =&gt; sub {
            my ( $kernel, $heap ) = @_[ KERNEL, HEAP ];

            # increment the step counter
            $heap-&gt;{init_step_counter}++ ;

            if ( 2 &lt;= $heap-&gt;{init_step_counter} ) {

                <font color="blue"># Here the initialization is complete and systemd gets</font>
                system(&quot;systemd-notify --ready&quot;);

                # kill this now useless session
               <font color="blue"> # Here we create an event and notify the kernel.</font>
               <font
              color="blue"> # yield means &quot;send the following event&quot; to the session</font>
              <font color="blue">  # we are currently in. As described above the kernel is</font>
               <font color="blue"> # involved in the handling of every event.</font>
                $kernel-&gt;yield(&quot;harakiri&quot;);
            }
        },
        harakiri =&gt; sub {
            # Actually no de-initialization code is needed here
            # because POE cleans up uneeded sessions automatically
        }
    }</pre>
            </div>
          </div>
        </div>
        <div style="margin-left: 35.145870328812px">
          <div style="text-indent: 0em">
            <div class="compact-block">
              <p>
                <tt class="verbatim">);</tt>
              </p>
            </div>
          </div>
        </div>
      </p>
    </div>
    <h1 id="auto-134">Appendix C<br></br>Content of the Accompanying Disk</h1>
    <p>
      Due to limitations of the compact disk file system (in regards to file
      name length and directory depth) the disk contains one big tar archive.
      On unix systems it can be unpacked using:
    </p>
    <pre class="verbatim" xml:space="preserve">
tar x -f BA_Anhang.tar</pre>
    <p>
      On windows it can be unpacked using the free software <tt class="verbatim">7zip</tt>.
    </p>
    <p>
      
    </p>
    <p>
      The archive contains several folders the two most important ones are: 
    </p>
    <dl>
      <p>
        <dt>
          Websites
        </dt>
        <dd>
          <p>
            Contains snapshots all online resources
          </p>
        </dd>
      </p>
      <p>
        <dt>
          Experimente
        </dt>
        <dd>
          <p>
            Contains the measured data of all experiments. Traffic dumps as
            well as generated diagrams and output of the performance
            measurements tools. 
          </p>
        </dd>
      </p>
    </dl>
    <h1 id="auto-135">Appendix D<br></br>Acknowledgements</h1>
    <p>
      After finishing this work I want to thank several people and entities:
    </p>
    <ol>
      <li>
        <p>
          Oliver Schoen and the company Inacon
        </p>
        <p>
          For letting me use the diagram of TCP Fast retransmit of their
          teaching material in my work.
        </p>
      </li>
      <li>
        <p>
          The Hackerspace Openlab Augsburg.
        </p>
        <p>
          For lending me the professional Gigabit switch supporting tagged
          VLANs, used in the refined experiments
        </p>
      </li>
      <li>
        <p>
          One important archaeology studying friend.
        </p>
        <p>
          Since she gave me personal backing and brought me food during the
          experiments.
        </p>
      </li>
    </ol>
    <h1 id="auto-136">Bibliography</h1>
    <div style="text-indent: 0em">
      <div class="compact-block">
        <font size="-1"><dl>
          <p>
            <p>
              <strong>[15]  </strong><a id="bib-man-tcp"></a>Linux Man Pages Project .
              <i>Linux Programmers Manuel - TCP (man 7 tcp)</i>. 12 2015.
            </p>
            <p>
              <strong>[APS99]  </strong><a id="bib-allman.paxson.stevens_2581rfc99"></a>M. Allman, V. Paxson, and
              W. Stevens. RFC 2581: TCP congestion control. 1999.
            </p>
            <p>
              <strong>[BAR+]  </strong><a id="bib-beckecomparison"></a>Martin Becke, Hakim Adhari,
              Erwin P Rathgeb, Fu Fa, Xiong Yang, and Xing Zhou. Comparison of
              multipath tcp and cmt-sctp based on intercontinental
              measurements.
            </p>
            <p>
              <strong>[Ben06]  </strong><a id="bib-benvenuti2006"></a>Christian Benvenuti.
              <i>Understanding Linux network internals</i>. O'Reilly,
              Sebastapol, Calif, 2006.
            </p>
            <p>
              <strong>[BHL+06]  </strong><a id="bib-bohacek2006new"></a>Stephan Bohacek, Joao P
              Hespanha, Junsoo Lee, Chansook Lim, and Katia Obraczka. A new
              tcp for persistent packet reordering. <i>IEEE/ACM Transactions
              on Networking (TON)</i>, 14(2):369&ndash;382, 2006.
            </p>
            <p>
              <strong>[Bon13]  </strong><a id="bib-mptcp-slides"></a>Olivier Bonaventure.
              Decoupling tcp from ip with multipath tcp. 2013.
            </p>
            <p>
              <strong>[Chr12]  </strong><a id="bib-christiansen2012programming"></a>Tom Christiansen.
              <i>Programming Perl</i>. O'Reilly, Sebastopol, CA, 2012.
            </p>
            <p>
              <strong>[dSF]  </strong><a id="bib-linux-packet-processing"></a>Bart de Schymer and Nick
              Fedchick. Ebtables/iptables interaction on a linux-based bridge.
              <a href="http://ebtables.netfilter.org/br_fw_ia/br_fw_ia.html"><tt>http://ebtables.netfilter.org/br_fw_ia/br_fw_ia.html</tt></a>.
            </p>
            <p>
              <strong>[Gmb]  </strong><a id="bib-alix-boxes-doc"></a>PC Engines GmbH. Alix boxes
              homepage. <a href="http://www.pcengines.ch/alix2d2.htm"><tt>http://www.pcengines.ch/alix2d2.htm</tt></a>.
            </p>
            <p>
              <strong>[Gmb13]  </strong><a id="bib-2013anordnung"></a>Viprinet Europe Gmbh.
              Anordnung zum &uuml;bermitteln eines datenstroms &uuml;ber
              geb&uuml;ndelte netzwerkzugangsleitungen, sowie sende- und
              empfangshilfsvorrichtung daf&uuml;r.
              https://www.google.com/patents/DE202008018396U1?cl=de,  11 2013.
              DE Patent 202,008,018,396.
            </p>
            <p>
              <strong>[HA10]  </strong><a id="bib-haldar2010operating"></a>S. Haldar and A.A. Aravind.
              <i>Operating Systems</i>. Pearson, 2010.
            </p>
            <p>
              <strong>[Ins81]  </strong><a id="bib-isi_793rfc81"></a>Information Sciences
              Institute. RFC 793. 1981. Edited by Jon Postel. Available at
              http://rfc.sunsite.dk/rfc/rfc793.html.
            </p>
            <p>
              <strong>[Kri]  </strong><a id="bib-sliding_window_webdoc"></a>John Kristoff. The
              transmission control protocoll. <a href="http://condor.depaul.edu/jkristof/technotes/tcp.html"><tt>http://condor.depaul.edu/jkristof/technotes/tcp.html</tt></a>.
            </p>
            <p>
              <strong>[Ran]  </strong><a id="bib-sctp-nat"></a>Irene Ruengeler Randall R.
              Stewart, Michael Tuexen. Stream control transmission protocol
              (sctp) network address translation support.
            </p>
            <p>
              <strong>[Riv]  </strong><a id="bib-linux-networking-overview"></a>Raoul Rivas. Linux kernel
              networking - some introductorily slides.
            </p>
            <p>
              <strong>[Ros13]  </strong><a id="bib-rosen2013linux"></a>R. Rosen. <i>Linux Kernel
              Networking: Implementation and Theory</i>. Books for
              professionals by professionals. Apress, 2013.
            </p>
            <p>
              <strong>[Sar02]  </strong><a id="bib-sarolahti2002linux"></a>Pasi Sarolahti. Linux tcp.
              <i>Nokia Research Centre</i>, 2002.
            </p>
            <p>
              <strong>[SFR04]  </strong><a id="bib-stevens2004unix"></a>W.R. Stevens, B. Fenner,
              and A.M. Rudoff. <i>UNIX Network Programming</i>. Number v. 1 in
              Addison-Wesley professional computing series. Addison-Wesley,
              2004.
            </p>
            <p>
              <strong>[Sny]  </strong><a id="bib-linux-packet-processing-img"></a>Joshua Snyder. Packetflow. <a
              href="http://ebtables.netfilter.org/br_fw_ia/PacketFlow.png"><tt>http://ebtables.netfilter.org/br_fw_ia/PacketFlow.png</tt></a>.
            </p>
            <p>
              <strong>[Ste93]  </strong><a id="bib-Stevens:1993:TIP"></a>W. Richard Stevens.
              <i>TCP/IP Illustrated (Vol. 1): The Protocols</i>.
              Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA,
              1993.
            </p>
            <p>
              <strong>[Ste07]  </strong><a id="bib-RFC4960"></a>R. Stewart. Stream control
              transmission protocol. Rfc 4960, RFC Editor, September 2007. <a
              href="http://www.rfc-editor.org/rfc/rfc4960.txt"><tt>http://www.rfc-editor.org/rfc/rfc4960.txt</tt></a>.
            </p>
            <p>
              <strong>[Tan03]  </strong><a id="bib-tanenbaum2003computer"></a>Andrew S Tanenbaum.
              Computer networks, 4-th edition. <i>Ed: Prentice Hall</i>, 2003.
            </p>
            <p>
              <strong>[tun]  </strong><a id="bib-tuntap-linux"></a>Linux kernel documentation on
              tun and tap devices. Documentation/networking/tuntap.txt file in
              Linux source tree.
            </p>
            <p>
              <strong>[WCB07]  </strong><a id="bib-wu2007performance"></a>Wenji Wu, Matt Crawford,
              and Mark Bowden. The performance analysis of linux
              networking&ndash;packet receiving. <i>Computer
              Communications</i>, 30(5):1044&ndash;1057, 2007.
            </p>
          </p>
        </dl></font>
      </div>
    </div>
    <h1 id="auto-137">List of figures</h1>
    <div style="text-indent: 0em">
      <div class="compact-block">
        <font size="-1"><p>
          Example of a 2-gateway Multipath Tunneling network setup. <span
          style="margin-left: 5mm"></span> <a href="#auto-5">9</a>
        </p><p>
          Diagram of the viprinet network(Fig.1) and encapsulation(Fig.2)
          concepts from the patent.  <span style="margin-left: 5mm"></span> <a href="#auto-12">12</a>
        </p><p>
          Multipath TCP Network Layout <span style="margin-left: 5mm"></span> <a href="#auto-17">13</a>
        </p><p>
          The Network architecture of the Experiment <span style="margin-left: 5mm"></span> <a
          href="#auto-28">17</a>
        </p><p>
          Photography of the Breadboard construction. (A concrete realisation
          of the testing network architecture in Figure <a href="#experiment-architecture">3.1</a>)
          <span style="margin-left: 5mm"></span> <a href="#auto-30">18</a>
        </p><p>
          Processing of Packets in the Linux Kernel (simplified) [<a href="#bib-linux-packet-processing">dSF</a>]
          [<a href="#bib-linux-packet-processing-img">Sny</a>] <span style="margin-left: 5mm"></span> <a href="#auto-33">19</a>
        </p><p>
          Explorative Experiment 1: full bidirectional Network I/O Graph,
          Y-Axis in Bytes I/O (payload) per second <span style="margin-left: 5mm"></span> <a
          href="#auto-37">20</a>
        </p><p>
          Explorative Experiment 1: bidirectional I/O Graph zoomed time-range
          : Second 16 to 28 <span style="margin-left: 5mm"></span> <a href="#auto-38">20</a>
        </p><p>
          Wireshark screenshot: packet details in previously discussed
          time-range. <span style="margin-left: 5mm"></span> <a href="#auto-39">20</a>
        </p><p>
          Explorative Experiment 2: Bidirectional I/O Graph <span style="margin-left: 5mm"></span>
          <a href="#auto-41">21</a>
        </p><p>
          Example of a simple TCP Connection <span style="margin-left: 5mm"></span> <a href="#auto-47">23</a>
        </p><p>
          Illustration of the sliding window principle[<a href="#bib-sliding_window_webdoc">Kri</a>] <span
          style="margin-left: 5mm"></span> <a href="#auto-51">24</a>
        </p><p>
          Illustration of the Fast Retransmit mechanics.[frt] <span style="margin-left: 5mm"></span>
          <a href="#auto-56">25</a>
        </p><p>
          Example of Fast Retransmit in network traffic. (Measured in
          wireshark) <span style="margin-left: 5mm"></span> <a href="#auto-57">26</a>
        </p><p>
          Diagram of the network interfaces and resources used by Multipath
          VPN <span style="margin-left: 5mm"></span> <a href="#auto-64">29</a>
        </p><p>
          A Simplified Comparison of tun/tap Interfaces to Conventional
          Network Interfaces <span style="margin-left: 5mm"></span> <a href="#auto-67">29</a>
        </p><p>
          Photography of the refined Experiment Setup <span style="margin-left: 5mm"></span> <a
          href="#auto-78">37</a>
        </p><p>
          Ascii art diagram of the refined test network architecture <span
          style="margin-left: 5mm"></span> <a href="#auto-79">38</a>
        </p><p>
          CPU Time Usage over Time on MTS and MTC compared Side by Side <span
          style="margin-left: 5mm"></span> <a href="#auto-99">43</a>
        </p><p>
          Side by Side comparison of CPU Time Usage on Tunnel Exit (MTS) and
          Data Throughput <span style="margin-left: 5mm"></span> <a href="#auto-104">45</a>
        </p><p>
          Throughput over time when disconnection one uplink for 20 seconds
          <span style="margin-left: 5mm"></span> <a href="#auto-114">47</a>
        </p><p>
          Network Throughput Diagram, Without any TCP Options <span style="margin-left: 5mm"></span>
          <a href="#auto-122">50</a>
        </p><p>
          Network Throughput Diagram, with Sender side modifications:
          tcp_reordering=15 and tcp_fack disabled <span style="margin-left: 5mm"></span> <a
          href="#auto-123">50</a>
        </p><p>
          Network Throughput Diagram, with Sender and Receiver-side
          modifications: tcp_reordering=15 and tcp_fack disabled <span style="margin-left: 5mm"></span>
          <a href="#auto-124">51</a>
        </p></font>
      </div>
    </div>
  </body>
</html>